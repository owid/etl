.. _quickstart:
.. highlight:: console

Quickstart
==========

The ETL can be used by OWID staff or the general public to build a copy of our data catalog. Here's how to get set up.

Getting set up
--------------

Dependencies
~~~~~~~~~~~~

The ETL is supported and regularly run on Linux, MacOS and Windows via WSL.

You will need:

- Python 3.9+. Guide for all platforms `here <https://realpython.com/installing-python/>`_. If you are using multiple different versions of Python on your machine, you may want to use ``pyenv`` to manage them (instructions `here <https://github.com/pyenv/pyenv>`_).
- ``poetry``, for managing dependencies, virtual envs, and packages. Installation guide `here <https://python-poetry.org/docs/#installation>`_.
- ``make``. You likely already have this, but otherwise it can be installed using your usual package manager
- MySQL client (and Python dev headers)

    - On Ubuntu: ``sudo apt install python3.9-dev mysql-client``
    - On Mac: ``brew install mysql-client``

OWID staff who want to add steps to the ETL will also need:

- AWS CLI installed (``pip install awscli``)
- An ``~/.aws/config`` file configured, so that you can upload to walden (ask someone to send you credentials)

.. code-block::

    [default]
    aws_access_key_id = <MY ACCESS KEY>
    aws_secret_access_key = <MY SECRET KEY>


Running tests
~~~~~~~~~~~~~

You can get started by using ``make`` to see available commands.

Run ``make test`` to check that your setup is working. It will clone two submodules in the ``vendor/`` folder, run ``poetry install``, and then run all CI checks.

If ``make test`` succeeds, then you should be able to build any dataset you like, including the entire catalog. If it fails, feel free to raise a `Github issue <https://github.com/owid/etl/issues>`_, or OWID staff can also ask using the ``#tech-issues`` Slack channel.

Before continuing, activate your Python virtual environment by running::

    $ source .venv/bin/activate

Building datasets
-----------------

Dry-runs
~~~~~~~~

Every step in the dag has a URI. For example, Our World In Data's population density dataset has the URI::

    data://garden/ggdc/2020-10-01/ggdc_maddison

We can see what steps would be executed to build it by running::

    $ etl --dry-run data://garden/ggdc/2020-10-01/ggdc_maddison
    Detecting which steps need rebuilding...
    Running 2 steps:
    1. walden://ggdc/2020-10-01/ggdc_maddison...
    2. data://garden/ggdc/2020-10-01/ggdc_maddison...

The first step is a ``walden://`` step, which when run will download an upstream snapshot of this dataset to the ``~/.owid/walden`` folder.

The second step is a ``data://`` step, which will generate a local dataset in the ``data/`` folder of the top-level ``etl/`` folder.

Observe that we can also skip the full path of the step, in which case it will do a regex match against all available steps::

    $ etl --dry-run ggdc_maddison

Now let's build the dataset, by removing the ``--dry-run`` option::

    $ etl data://garden/ggdc/2020-10-01/ggdc_maddison
    Detecting which steps need rebuilding...
    Running 2 steps:
    1. walden://ggdc/2020-10-01/ggdc_maddison...
    OK (2s)

    2. data://garden/ggdc/2020-10-01/ggdc_maddison...
    OK (4s)

Let's confirm that the dataset was built locally::

    $ ls data/garden/ggdc/2020-10-01/ggdc_maddison/
    index.json
    maddison_gdp.feather
    maddison_gdp.meta.json
    maddison_gdp.parquet

Several files got built for the dataset. The first is ``index.json`` which gives metadata about the whole dataset. The remaining three files all represent a single data table, which is saved in both Feather and Parquet formats.

Consuming data
--------------

.. highlight:: pycon

Now that our ``data/`` folder has a table built, we can try reading it.  Let's run ``python`` and use Pandas::

    >>> import pandas as pd
    >>> df = pd.read_feather('data/garden/ggdc/2020-10-01/ggdc_maddison/maddison_gdp.feather')
    >>> df.head()
        country  year  gdp_per_capita  population           gdp
    0  Afghanistan  1820             NaN   3280000.0           NaN
    1  Afghanistan  1870             NaN   4207000.0           NaN
    2  Afghanistan  1913             NaN   5730000.0           NaN
    3  Afghanistan  1950          1156.0   8150000.0  9.421400e+09
    4  Afghanistan  1951          1170.0   8284000.0  9.692280e+09

We can see that this dataset provides three indicators (``gdp``, ``population``, and ``gdp_per_capita``), reported by country and year.

All tables generated by the ETL can also be read and written using a wrapper around Pandas, the ``Table`` class. If we read the table using that, it will also pick up the metadata that was in the ``.meta.json`` file.

::

    >>> from owid.catalog import Table
    >>> t = Table.read('data/garden/ggdc/2020-10-01/ggdc_maddison/maddison_gdp.feather')
    >>> t.head()
                    gdp_per_capita  population           gdp
    country     year
    Afghanistan 1820             NaN   3280000.0           NaN
                1870             NaN   4207000.0           NaN
                1913             NaN   5730000.0           NaN
                1950          1156.0   8150000.0  9.421400e+09
                1951          1170.0   8284000.0  9.692280e+09

In this case, we can see that it understood that ``country`` and ``year`` columns were the primary key for this table, and put them in the index.
