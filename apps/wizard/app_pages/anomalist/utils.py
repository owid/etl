"""Utils for chart revision tool."""
from typing import Any, Dict, List, Tuple

import pandas as pd
import streamlit as st
from pymysql import OperationalError
from structlog import get_logger

from apps.wizard.utils.cached import get_datasets_from_version_tracker
from apps.wizard.utils.db import WizardDB
from etl.grapher_io import get_all_datasets

# Logger
log = get_logger()


# TODO: Consider refactoring the following function, which does too many things.
@st.spinner("Retrieving datasets...")
def get_datasets_and_mapping_inputs() -> Tuple[pd.DataFrame, List[Dict[str, Dict[str, Any]]], Dict[int, int]]:
    # NOTE: The following ignores DB datasets that are archived (which is a bit unexpected).
    # I had to manually un-archive the testing datasets from the database manually to make things work.
    # This could be fixed, but maybe it's not necessary (since we won't archive an old version of a dataset until the
    # new has been analyzed).
    steps_df_grapher, grapher_changes = get_datasets_from_version_tracker()

    # List new dataset ids based on changes in files.
    datasets_new_ids = [ds["new"]["id"] for ds in grapher_changes]

    # Combine with datasets from database that are not present in ETL
    # Get datasets from Database
    try:
        datasets_db = get_all_datasets(archived=True)
    except OperationalError as e:
        raise OperationalError(
            f"Could not retrieve datasets. Try reloading the page. If the error persists, please report an issue. Error: {e}"
        )

    # Get table with all datasets (ETL + DB)
    steps_df_grapher = (
        steps_df_grapher.merge(datasets_db, on="id", how="outer", suffixes=("_etl", "_db"))
        .sort_values(by="id", ascending=False)
        .drop(columns="updatedAt")
        .astype({"id": int})
    )
    columns = ["namespace", "name"]
    for col in columns:
        steps_df_grapher[col] = steps_df_grapher[f"{col}_etl"].fillna(steps_df_grapher[f"{col}_db"])
        steps_df_grapher = steps_df_grapher.drop(columns=[f"{col}_etl", f"{col}_db"])

    assert steps_df_grapher["name"].notna().all(), "NaNs found in `name`"
    assert steps_df_grapher["namespace"].notna().all(), "NaNs found in `namespace`"

    # Replace NaN with empty string in etl paths (otherwise dataset won't be shown if 'show step names' is chosen)
    steps_df_grapher["step"] = steps_df_grapher["step"].fillna("")

    # Add a convenient column for "[dataset id] Dataset Name"
    steps_df_grapher["id_name"] = [f"[{ds['id']}] {ds['name']}" for ds in steps_df_grapher.to_dict(orient="records")]

    # Load mapping created by indicator upgrader (if any).
    mapping = WizardDB.get_variable_mapping_raw()
    if len(mapping) > 0:
        # Set of ids of new datasets that appear in the mapping generated by indicator upgrader.
        datasets_new_mapped = set(mapping["dataset_id_new"])
        # Set of ids of datasets that have appear as new datasets in the grapher_changes.
        datasets_new_expected = set(datasets_new_ids)
        # Sanity check.
        if not (datasets_new_mapped <= datasets_new_expected):
            log.error(
                f"Indicator upgrader mapped indicators to new datasets ({datasets_new_mapped}) that are not among the datasets detected as new in the code ({datasets_new_expected}). Look into this."
            )
        # Create a mapping dictionary.
        variable_mapping = mapping.set_index("id_old")["id_new"].to_dict()
        # Sanity check.
        # TODO: Remove this check once we're sure that this works properly (to save time).
        assert variable_mapping == WizardDB.get_variable_mapping(), "Unexpected mapping issues."
    else:
        # NOTE: Here we could also infer the mapping of the new datasets (assuming no names have changed).
        #  This could be useful if a user wants to compare two arbitrary versions of existing grapher datasets.
        variable_mapping = dict()

    # List all grapher datasets.
    datasets_all = steps_df_grapher["id_name"].to_list()

    # List new datasets.
    datasets_new = [
        ds_id_name for ds_id_name in datasets_all if int(ds_id_name[1 : ds_id_name.index("]")]) in datasets_new_ids
    ]

    return datasets_all, datasets_new, variable_mapping  # type: ignore
