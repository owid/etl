"""Utils for chart revision tool."""
from enum import Enum
from typing import Dict, List, Tuple

import pandas as pd
import streamlit as st
from sqlalchemy.orm import Session
from structlog import get_logger

import etl.grapher_model as gm
from apps.anomalist.anomalist_api import add_auxiliary_scores, combine_and_reduce_scores_df
from apps.wizard.utils.db import WizardDB
from apps.wizard.utils.io import get_new_grapher_datasets_and_their_previous_versions
from etl.config import OWID_ENV, OWIDEnv
from etl.db import get_engine

# Logger
log = get_logger()


class AnomalyTypeEnum(Enum):
    TIME_CHANGE = "time_change"
    UPGRADE_CHANGE = "upgrade_change"
    UPGRADE_MISSING = "upgrade_missing"
    GP_OUTLIER = "gp_outlier"
    # AI = "ai"  # Uncomment if needed


def infer_variable_mapping(dataset_id_new: int, dataset_id_old: int) -> Dict[int, int]:
    engine = get_engine()
    with Session(engine) as session:
        variables_new = gm.Variable.load_variables_in_datasets(session=session, dataset_ids=[dataset_id_new])
        variables_old = gm.Variable.load_variables_in_datasets(session=session, dataset_ids=[dataset_id_old])
    # Create a mapping from old ids to new variable ids for variables whose shortNames are identical in the old and new versions.
    _variables = {variable.shortName: variable.id for variable in variables_new}
    variable_mapping = {
        old_variable.id: _variables[old_variable.shortName]
        for old_variable in variables_old
        if old_variable.shortName in _variables
    }
    return variable_mapping


@st.cache_data(show_spinner=False)
@st.spinner("Retrieving datasets...")
def get_datasets_and_mapping_inputs() -> Tuple[Dict[int, str], Dict[int, str], Dict[int, int]]:
    # Get all datasets from DB.
    df_datasets = gm.Dataset.load_all_datasets()

    # Detect local files that correspond to new or modified grapher steps, identify their corresponding grapher dataset ids, and the grapher dataset id of the previous version (if any).
    dataset_new_and_old = get_new_grapher_datasets_and_their_previous_versions()

    # List new dataset ids.
    datasets_new_ids = list(dataset_new_and_old)

    # Load mapping created by indicator upgrader (if any).
    mapping = WizardDB.get_variable_mapping_raw()
    if len(mapping) > 0:
        log.info("Using variable mapping created by indicator upgrader.")
        # Set of ids of new datasets that appear in the mapping generated by indicator upgrader.
        datasets_new_mapped = set(mapping["dataset_id_new"])
        # Set of ids of expected new datasets.
        datasets_new_expected = set(datasets_new_ids)
        # Sanity check.
        if not (datasets_new_mapped <= datasets_new_expected):
            log.error(
                f"Indicator upgrader mapped indicators to new datasets ({datasets_new_mapped}) that are not among the datasets detected as new in the code ({datasets_new_expected}). Look into this."
            )
        # Create a mapping dictionary.
        variable_mapping = mapping.set_index("id_old")["id_new"].to_dict()
    else:
        log.info("Inferring variable mapping (since no mapping was created by indicator upgrader).")
        # Infer the mapping of the new datasets (assuming no names have changed).
        variable_mapping = dict()
        for dataset_id_new, dataset_id_old in dataset_new_and_old.items():
            if dataset_id_old is None:
                continue
            # Infer
            variable_mapping.update(infer_variable_mapping(dataset_id_new, dataset_id_old))

    # For convenience, create a dataset name "[id] Name".
    df_datasets["id_name"] = "[" + df_datasets["id"].astype(str) + "] " + df_datasets["name"]
    # List all grapher datasets.
    datasets_all = df_datasets[["id", "id_name"]].set_index("id").squeeze().to_dict()

    # List new datasets.
    datasets_new = {k: v for k, v in datasets_all.items() if k in datasets_new_ids}

    return datasets_all, datasets_new, variable_mapping  # type: ignore


def create_tables(_owid_env: OWIDEnv = OWID_ENV):
    """Create all required tables.

    If exist, nothing is created.
    """
    gm.Anomaly.create_table(_owid_env.engine)


@st.cache_data(show_spinner=False)
def get_scores(anomalies: List[gm.Anomaly]) -> pd.DataFrame:
    """Combine and reduce scores dataframe."""
    df = combine_and_reduce_scores_df(anomalies)

    # Add a population score, an analytics score, and a weighted score.
    df = add_auxiliary_scores(df=df)

    return df
