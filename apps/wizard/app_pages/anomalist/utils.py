"""Utils for chart revision tool."""
from enum import Enum
from typing import Dict, List, Tuple

import pandas as pd
import streamlit as st
from sqlalchemy.orm import Session
from structlog import get_logger

import etl.grapher_model as gm
from apps.anomalist.anomalist_api import add_auxiliary_scores, combine_and_reduce_scores_df
from apps.wizard.utils.db import WizardDB
from apps.wizard.utils.io import get_new_grapher_datasets_and_their_previous_versions
from etl.config import OWID_ENV, OWIDEnv
from etl.db import get_engine

# Logger
log = get_logger()


class AnomalyTypeEnum(Enum):
    TIME_CHANGE = "time_change"
    UPGRADE_CHANGE = "upgrade_change"
    UPGRADE_MISSING = "upgrade_missing"
    GP_OUTLIER = "gp_outlier"
    # AI = "ai"  # Uncomment if needed


# TODO: Move elsewhere. Note that there is get_all_datasets in grapher_io, but it doesn't return all necessary columns.
def get_all_grapher_datasets():
    # Initialize database engine.
    engine = get_engine()
    # Load all relevant grapher datasets from DB.
    with Session(engine) as session:
        datasets = session.query(gm.Dataset).all()
    df_datasets = pd.DataFrame(datasets)

    return df_datasets


@st.cache_data(show_spinner=False)
@st.spinner("Retrieving datasets...")
def get_datasets_and_mapping_inputs() -> Tuple[Dict[int, str], Dict[int, str], Dict[int, int]]:
    # Get all datasets from DB.
    # NOTE: Currently, we are including archived datasets, but I think that may be good.
    df_datasets = get_all_grapher_datasets()

    # Detect local files that correspond to new or modified grapher steps, identify their corresponding grapher dataset ids, and the grapher dataset id of the previous version (if any).
    dataset_new_and_old = get_new_grapher_datasets_and_their_previous_versions()

    # List new dataset ids.
    datasets_new_ids = list(dataset_new_and_old)

    # Load mapping created by indicator upgrader (if any).
    mapping = WizardDB.get_variable_mapping_raw()
    if len(mapping) > 0:
        # Set of ids of new datasets that appear in the mapping generated by indicator upgrader.
        datasets_new_mapped = set(mapping["dataset_id_new"])
        # Set of ids of expected new datasets.
        datasets_new_expected = set(datasets_new_ids)
        # Sanity check.
        if not (datasets_new_mapped <= datasets_new_expected):
            log.error(
                f"Indicator upgrader mapped indicators to new datasets ({datasets_new_mapped}) that are not among the datasets detected as new in the code ({datasets_new_expected}). Look into this."
            )
        # Create a mapping dictionary.
        variable_mapping = mapping.set_index("id_old")["id_new"].to_dict()
    else:
        # NOTE: Here we could also infer the mapping of the new datasets (assuming no names have changed).
        #  This could be useful if a user wants to compare two arbitrary versions of existing grapher datasets.
        # TODO: This could now be easily achieved.
        variable_mapping = dict()

    # For convenience, create a dataset name "[id] Name".
    df_datasets["id_name"] = "[" + df_datasets["id"].astype(str) + "] " + df_datasets["name"]
    # List all grapher datasets.
    datasets_all = df_datasets[["id", "id_name"]].set_index("id").squeeze().to_dict()

    # List new datasets.
    datasets_new = {k: v for k, v in datasets_all.items() if k in datasets_new_ids}

    return datasets_all, datasets_new, variable_mapping  # type: ignore


def create_tables(_owid_env: OWIDEnv = OWID_ENV):
    """Create all required tables.

    If exist, nothing is created.
    """
    gm.Anomaly.create_table(_owid_env.engine)


@st.cache_data(show_spinner=False)
def get_scores(anomalies: List[gm.Anomaly]) -> pd.DataFrame:
    """Combine and reduce scores dataframe."""
    df = combine_and_reduce_scores_df(anomalies)

    # Add a population score, an analytics score, and a weighted score.
    df = add_auxiliary_scores(df=df)

    return df
