"""Utils for chart revision tool."""
import time
from enum import Enum
from typing import Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st
from sqlalchemy import select
from sqlalchemy.orm import Session
from structlog import get_logger

import etl.grapher_model as gm
from apps.anomalist.anomalist_api import add_auxiliary_scores, combine_and_reduce_scores_df
from apps.wizard.utils.db import WizardDB
from apps.wizard.utils.io import get_new_grapher_datasets_and_their_previous_versions
from etl.config import OWID_ENV, OWIDEnv
from etl.db import get_engine

# Logger
log = get_logger()


class AnomalyTypeEnum(Enum):
    TIME_CHANGE = "time_change"
    UPGRADE_CHANGE = "upgrade_change"
    UPGRADE_MISSING = "upgrade_missing"
    GP_OUTLIER = "gp_outlier"
    # AI = "ai"  # Uncomment if needed


def infer_variable_mapping(dataset_id_new: int, dataset_id_old: int) -> Dict[int, int]:
    engine = get_engine()
    with Session(engine) as session:
        variables_new = gm.Variable.load_variables_in_datasets(session=session, dataset_ids=[dataset_id_new])
        variables_old = gm.Variable.load_variables_in_datasets(session=session, dataset_ids=[dataset_id_old])
    # Create a mapping from old ids to new variable ids for variables whose shortNames are identical in the old and new versions.
    _variables = {variable.shortName: variable.id for variable in variables_new}
    variable_mapping = {
        old_variable.id: _variables[old_variable.shortName]
        for old_variable in variables_old
        if old_variable.shortName in _variables
    }
    return variable_mapping


@st.cache_data(show_spinner=False)
@st.spinner("Retrieving datasets...")
def get_datasets_and_mapping_inputs() -> Tuple[Dict[int, str], Dict[int, str], Dict[int, int]]:
    t = time.time()
    # Get all datasets from DB.
    df_datasets = gm.Dataset.load_all_datasets(columns=["id", "name"])

    # Initialize DB engine.
    engine = get_engine()
    with Session(engine) as session:
        # Ensure the 'anomalies' table exists.
        gm.Anomaly.create_table(engine, if_exists="skip")

        # Get list of datasets for which anomalies have already been detected (if any).
        dataset_ids_with_anomalies = sorted(set(session.scalars(select(gm.Anomaly.datasetId)).all()))

        # Detect local files that correspond to new or modified grapher steps, identify their corresponding grapher dataset ids, and the grapher dataset id of the previous version (if any).
        # NOTE: this is quite slow taking ~4s, it would be faster to reuse function `_load_datasets_new_ids` from owidbot/anomalist.py
        dataset_new_and_old = get_new_grapher_datasets_and_their_previous_versions(session=session)

    # List new dataset ids.
    # Add datasets with already detected anomalies (if any).
    datasets_new_ids = list(dataset_new_and_old) + dataset_ids_with_anomalies

    # Load mapping created by indicator upgrader (if any).
    variable_mapping = load_variable_mapping(datasets_new_ids, dataset_new_and_old)

    # For convenience, create a dataset name "[id] Name".
    df_datasets["id_name"] = "[" + df_datasets["id"].astype(str) + "] " + df_datasets["name"]
    # List all grapher datasets.
    datasets_all = df_datasets[["id", "id_name"]].set_index("id").squeeze().to_dict()
    # List new datasets.
    datasets_new = {k: v for k, v in datasets_all.items() if k in datasets_new_ids}

    log.info("get_datasets_and_mapping_inputs", t=time.time() - t)

    return datasets_all, datasets_new, variable_mapping  # type: ignore


def load_variable_mapping(
    datasets_new_ids: List[int], dataset_new_and_old: Optional[Dict[int, Optional[int]]] = None
) -> Dict[int, int]:
    mapping = WizardDB.get_variable_mapping_raw()
    if len(mapping) > 0:
        log.info("Using variable mapping created by indicator upgrader.")
        # Set of ids of new datasets that appear in the mapping generated by indicator upgrader.
        datasets_new_mapped = set(mapping["dataset_id_new"])
        # Set of ids of expected new datasets.
        datasets_new_expected = set(datasets_new_ids)
        # Sanity check.
        if not (datasets_new_mapped <= datasets_new_expected):
            log.error(
                f"Indicator upgrader mapped indicators to new datasets ({datasets_new_mapped}) that are not among the datasets detected as new in the code ({datasets_new_expected}). Look into this."
            )
        # Create a mapping dictionary.
        variable_mapping = mapping.set_index("id_old")["id_new"].to_dict()
    elif dataset_new_and_old:
        log.info("Inferring variable mapping (since no mapping was created by indicator upgrader).")
        # Infer the mapping of the new datasets (assuming no names have changed).
        variable_mapping = dict()
        for dataset_id_new, dataset_id_old in dataset_new_and_old.items():
            if dataset_id_old is None:
                continue
            # Infer
            variable_mapping.update(infer_variable_mapping(dataset_id_new, dataset_id_old))
    else:
        # No mapping available.
        variable_mapping = dict()

    return variable_mapping  # type: ignore


def create_tables(_owid_env: OWIDEnv = OWID_ENV):
    """Create all required tables.

    If exist, nothing is created.
    """
    gm.Anomaly.create_table(_owid_env.engine, if_exists="skip")


@st.cache_data(show_spinner=False)
def get_scores(anomalies: List[gm.Anomaly]) -> pd.DataFrame:
    """Combine and reduce scores dataframe."""
    df = combine_and_reduce_scores_df(anomalies)

    # Add a population score, an analytics score, and a weighted score.
    df = add_auxiliary_scores(df=df)

    return df
