"""Logic and code to combine multiple collections (MDIMs or Explorers) into a single one.

Additional: combine dimensions (using raw dictionaries)
"""

from copy import deepcopy
from typing import Any, Dict, List, Mapping, Optional, Set, Tuple, TypeVar, Union, overload

import pandas as pd
from structlog import get_logger

from etl.collection.core.utils import create_collection_from_config
from etl.collection.explorer import Explorer
from etl.collection.model.core import Collection
from etl.collection.model.dimension import Dimension, DimensionChoice
from etl.collection.utils import records_to_dictionary

log = get_logger()

COLLECTION_SLUG = "collection__slug"
COLLECTION_TITLE = "Collection"

# Define type variables to use in overloads
T = TypeVar("T", bound=Collection)
E = TypeVar("E", bound=Explorer)


# COMBINE DIMENSIONS
def combine_config_dimensions(
    config_dimensions: List[Dict[str, Any]],
    config_dimensions_yaml: List[Dict[str, Any]],
    choices_top: bool = False,
    dimensions_top: bool = False,
):
    """Combine the dimension configuration from the YAML file with the one generated programmatically.

    There are various strategies that we could follow here, but currently:

    - We consider the union of config_dimensions (returned by expander.build_dimensions) nad config_dimensions_yaml.
    - These are kept as-is, unless they are in the YML config, in which case they are overwritten.

    Other possible strategies:

    - We could do the reverse, and only consider the fields from config_dimensions_yaml. I'm personally unsure when this could be valuable.


    Arguments
    ---------
    config_dimensions: List[Dict[str, Any]]
        Generated by expander.build_dimensions.
    config_dimensions_yaml:  List[Dict[str, Any]]
        From the YAML file.
    choices_top: bool
        Set to True to place the choices from `config_dimensions` first.
    dimensions_top: bool
        Set to True to place the dimensions from `config_dimensions` first.

    TODO:

        - I think we need to add more checks to ensure that there is nothing weird being produced here.
    """

    config_dimensions_combined = deepcopy(config_dimensions)
    dims_overwrite = records_to_dictionary(config_dimensions_yaml, "slug")

    # Overwrite dimensions
    for dim in config_dimensions_combined:
        slug_dim = dim["slug"]
        if slug_dim in dims_overwrite:
            # Get dimension data to overwrite, remove it from dictionary
            dim_overwrite = dims_overwrite.pop(slug_dim)

            # Overwrite dimension name
            dim["name"] = dim_overwrite.get("name", dim["name"])

            # Overwrite presentation
            if "presentation" in dim_overwrite:
                dim["presentation"] = dim_overwrite["presentation"]

            # Overwrite choices
            if "choices" in dim_overwrite:
                choices_overwrite = records_to_dictionary(
                    dim_overwrite["choices"],
                    "slug",
                )
                assert "choices" in dim, (
                    f"Choices not found in dimension: {dim}! This is rare, please report this issue!"
                )
                for choice in dim["choices"]:
                    slug_choice = choice["slug"]
                    if slug_choice in choices_overwrite:
                        # Get dimension data to overwrite, remove it from dictionary
                        choice_overwrite = choices_overwrite.pop(slug_choice)

                        # Overwrite choice name
                        choice["name"] = choice_overwrite.get("name", dim["name"])
                        # Overwrite choice description
                        choice["description"] = choice_overwrite.get("description", choice["description"])
                        # Overwrite group
                        group = choice_overwrite.get("group")
                        if group is not None:
                            choice["group"] = group

                # Handle choices from YAML not present in config_dimensions
                if choices_overwrite:
                    missing_choices = []
                    for slug, values in choices_overwrite.items():
                        choice = {"slug": slug, **values}
                        missing_choices.append(choice)

                    if choices_top:
                        dim["choices"] += missing_choices
                    else:
                        dim["choices"] = missing_choices + dim["choices"]

                # Sort choices based on how these appear in the YAML file (only if dimensions_top is False)
                if not choices_top:
                    dim["choices"] = _order(dim_overwrite["choices"], dim["choices"])

    # Handle dimensions from YAML not present in config_dimensions
    if dims_overwrite:
        missing_dims = []
        for slug, values in dims_overwrite.items():
            dim = {"slug": slug, **values}
            missing_dims.append(dim)

        if dimensions_top:
            config_dimensions_combined += missing_dims
        else:
            config_dimensions_combined = missing_dims + config_dimensions_combined

    # Sort dimensions based on how these appear in the YAML file (only if dimensions_top is False)
    if not dimensions_top:
        config_dimensions_combined = _order(config_dimensions_yaml, config_dimensions_combined)

    return config_dimensions_combined


def _order(config_yaml, config_combined):
    # Build score
    score = {record["slug"]: i for i, record in enumerate(config_yaml)}
    # Split: those that need ordering, those that don't
    config_sort = [record for record in config_combined if record["slug"] in score]
    config_others = [record for record in config_combined if record["slug"] not in score]

    # Order if applicable
    config_sort = sorted(
        config_sort,
        key=lambda x: score.get(x["slug"], 100),
    )

    return config_sort + config_others


@overload
def combine_collections(
    collections: List[E],
    collection_name: str,
    config: Optional[Dict[str, Any]] = None,
    dependencies: Optional[Set[str]] = None,
    force_collection_dimension: bool = False,
    collection_dimension_name: Optional[str] = None,
    collection_choices_names: Optional[List[str]] = None,
    is_explorer: Optional[bool] = None,
) -> E: ...


@overload
def combine_collections(
    collections: List[T],
    collection_name: str,
    config: Optional[Dict[str, Any]] = None,
    dependencies: Optional[Set[str]] = None,
    force_collection_dimension: bool = False,
    collection_dimension_name: Optional[str] = None,
    collection_choices_names: Optional[List[str]] = None,
    is_explorer: Optional[bool] = None,
) -> T: ...


# COMBINE COLLECTIONS
def combine_collections(
    collections: Union[List[Collection], List[Explorer]],
    collection_name: str,
    config: Optional[Dict[str, Any]] = None,
    dependencies: Optional[Set[str]] = None,
    force_collection_dimension: bool = False,
    collection_dimension_name: Optional[str] = None,
    collection_choices_names: Optional[List[str]] = None,
    is_explorer: Optional[bool] = None,
) -> Union[Collection, Explorer]:
    """Combine multiple collections (MDIMs or Explorers) into a single one.

    This function serves as a unified interface to combine either Explorers
    or MDIMs (Collections), abstracting the common logic between the two.

    Args:
        collections: List of collections (either all MDIMs or all Explorers) to combine
        collection_name: Name of the resulting combined collection
        config: Configuration for the combined collection
        dependencies: Set of dependencies for the combined collection
        force_collection_dimension: If True, adds a dimension to identify the source collection
            even if there are no duplicate views
        collection_dimension_name: Name for the dimension that identifies the source collection
            (defaults to "MDIM" for Collections or "Explorer" for Explorers)
        collection_choices_names: Names for the choices in the source dimension
            (should match the length of collections)
        is_explorer: Force the result to be an Explorer (True) or MDIM (False).
            If None (default), inferred from the input collections.

    Returns:
        A combined Collection or Explorer, matching the type of the input collections

    Notes:
        - All collections must have the same dimensions structure (slug, name, etc.)
        - Choice conflicts are resolved by renaming conflicting choices
        - If duplicate views exist, a source dimension is automatically added
    """
    # Check that there are at least 2 collections to combine
    assert len(collections) > 0, "No collections to combine."
    assert len(collections) > 1, "At least two collections should be provided."

    # Determine collection type if not specified
    if is_explorer is None:
        is_explorer = all(isinstance(c, Explorer) for c in collections)
        if not (is_explorer or all(not isinstance(c, Explorer) for c in collections)):
            raise ValueError("All collections must be of the same type (either all Explorers or all Collections)")

    # Set appropriate default dimension name based on collection type
    if collection_dimension_name is None:
        collection_dimension_name = COLLECTION_TITLE

    # Check that all collections have the same dimensions structure
    collection_dims = None
    for collection in collections:
        dimensions_flatten = [
            {k: v for k, v in dim.to_dict().items() if k != "choices"} for dim in collection.dimensions
        ]
        if collection_dims is None:
            collection_dims = dimensions_flatten
        else:
            assert collection_dims == dimensions_flatten, (
                "Dimensions are not the same across collections. Please review that dimensions are listed in the same order, have the same slugs, names, description, etc."
            )

    # Check for checkbox dimensions in the first collection
    # TODO: Implement support for checkboxes when merging
    for dim in collections[0].dimensions:
        if dim.ui_type == "checkbox" and is_explorer:
            raise NotImplementedError("Checkbox dimensions are not supported yet for Explorers.")

    # Detect duplicate views
    seen_dims = set()
    has_duplicate_views = False
    for collection in collections:
        # duplicate views within a collection
        collection.check_duplicate_views()
        # duplicate views across collections
        for view in collection.views:
            dims = tuple(view.dimensions.items())
            if dims in seen_dims:
                has_duplicate_views = True
                break
            seen_dims.add(dims)

    # Add collection dimension if needed
    if has_duplicate_views or force_collection_dimension:
        for i, collection in enumerate(collections):
            if collection_choices_names is not None:
                assert len(collection_choices_names) == len(collections), (
                    "Length of collection_choices_names must match the number of collections"
                )
                choice_name = collection_choices_names[i]
            else:
                choice_name = collection.title.get("title", collection.short_name)

            dimension_collection = Dimension(
                slug=COLLECTION_SLUG,
                name=collection_dimension_name,
                choices=[
                    DimensionChoice(slug=collection.short_name, name=choice_name),
                ],
            )
            collection.dimensions = [dimension_collection] + collection.dimensions
            for v in collection.views:
                v.dimensions[COLLECTION_SLUG] = collection.short_name

    # Create dictionary with collections for tracking
    collections_by_id = {str(i): deepcopy(collection) for i, collection in enumerate(collections)}

    # Build dataframe with all choices
    df_choices, cols_choices = _build_df_choices(collections_by_id)

    # Combine dimensions (use first collection as template)
    dimensions = _combine_dimensions(
        df_choices=df_choices,
        cols_choices=cols_choices,
        collection=collections[0].copy(),
    )

    # Track modifications (useful later for views)
    choice_slug_changes = _extract_choice_slug_changes(df_choices)

    # Update views based on changes to choice slugs
    collections_by_id = _update_choice_slugs_in_views(choice_slug_changes, collections_by_id)

    # Collect all views
    views = []
    for _, collection in collections_by_id.items():
        views.extend(collection.views)

    # Create catalog path
    assert isinstance(collections[0].catalog_path, str), "Catalog path is not set. Please set it before saving."
    catalog_path = collections[0].catalog_path.split("#")[0] + "#" + collection_name

    # Ensure config has minimal required fields
    if config is None:
        cconfig = {}
    else:
        cconfig = deepcopy(config)

    # Make sure there is title and default_selection. If not given, use default values.
    default_title = {
        "title": f"Combined Collection: {collection_name}",
        "title_variant": "Use a YAML to define these attributes",
    }
    if not is_explorer:
        if "title" not in cconfig:
            cconfig["title"] = default_title
        else:
            cconfig["title"] = {**default_title, **cconfig["title"]}
        if "default_selection" not in cconfig:
            cconfig["default_selection"] = collections[0].default_selection
    else:
        if "config" not in cconfig:
            cconfig["config"] = {}
        if "explorerTitle" not in cconfig["config"]:
            cconfig["config"]["explorerTitle"] = default_title["title"]
        if "explorerSubtitle" not in cconfig["config"]:
            cconfig["config"]["explorerSubtitle"] = default_title["title_variant"]

    # Set dimensions and views
    cconfig["dimensions"] = dimensions
    cconfig["views"] = views

    # Create the combined collection
    combined = create_collection_from_config(
        config=cconfig,
        dependencies=dependencies if dependencies is not None else set(),
        catalog_path=catalog_path,
        validate_schema=True if not is_explorer else False,
        explorer=is_explorer,
    )

    # Log any conflicts that were resolved
    df_conflict = df_choices.loc[df_choices["in_conflict"]]
    if not df_conflict.empty:
        log.warning("Choice slug conflicts resolved")
        for (dimension_slug, choice_slug), group in df_conflict.groupby(["dimension_slug", "slug_original"]):
            log.warning(f"(dimension={dimension_slug}, choice={choice_slug})")
            for _, subgroup in group.groupby("choice_slug_id"):
                collection_ids = subgroup["collection_id"].unique().tolist()
                collection_names = [collections_by_id[i].short_name for i in collection_ids]
                record = subgroup[cols_choices].drop_duplicates().to_dict("records")
                assert len(record) == 1, "Unexpected, please report!"
                log.warning(f" Collections {collection_names} map to {record[0]}")

    return combined


def _extract_choice_slug_changes(df_choices) -> Dict[str, Any]:
    # Track modifications (useful later for views)
    slug_changes = (
        df_choices.loc[df_choices["in_conflict"]]
        .groupby(["collection_id", "dimension_slug"])
        .apply(lambda x: dict(zip(x["slug_original"], x["slug"])), include_groups=False)
        .unstack("collection_id")
        .to_dict()
    )

    return slug_changes


def _combine_dimensions(
    df_choices: pd.DataFrame, cols_choices: List[str], collection: Union[Explorer, Collection]
) -> List[Dimension]:
    """Combine dimensions from different explorers"""
    # Dimension bucket
    dimensions = collection.dimensions.copy()

    # Drop duplicates
    df_choices = df_choices.drop_duplicates(subset=cols_choices + ["slug", "dimension_slug"])

    # Iterate over each dimension and update the list of choices
    for dimension in dimensions:
        df_dim_choices = df_choices.loc[
            df_choices["dimension_slug"] == dimension.slug, cols_choices + ["slug"]
        ].drop_duplicates()

        assert len(df_dim_choices) == df_dim_choices["slug"].nunique(), (
            f"Duplicate slugs in dimension {dimension.slug} choices."
        )

        # Raw choices
        choices = df_dim_choices.to_dict("records")

        # Build choices
        dimension.choices = [DimensionChoice.from_dict(c) for c in choices]

    return dimensions


def _update_choice_slugs_in_views(choice_slug_changes, collection_by_id) -> Mapping[str, Union[Collection, Explorer]]:
    """Access each explorer, and update choice slugs in views"""
    for collection_id, change in choice_slug_changes.items():
        # Get collection
        collection = collection_by_id[collection_id]

        # Get views as dataframe for easy processing
        df_views_dimensions = pd.DataFrame([view.dimensions for view in collection.views])

        # FUTURE: this needs to change in order to support checkboxes
        df_views_dimensions = df_views_dimensions.astype("string")

        # Process views
        df_views_dimensions = df_views_dimensions.replace(change)

        # Bring back views to collections
        views_dimensions = df_views_dimensions.to_dict("records")
        for view, view_dimensions in zip(collection.views, views_dimensions):
            # cast keys to str to satisfy type requirements
            view.dimensions = {str(key): value for key, value in view_dimensions.items()}
    return collection_by_id


def _build_df_choices(collections_by_id: Mapping[str, Union[Collection, Explorer]]) -> Tuple[pd.DataFrame, List[str]]:
    # Collect all choices in a dataframe: choice_slug, choice_name, ..., collection_id, dimension_slug.
    records = []
    for i, explorer in collections_by_id.items():
        for dim in explorer.dimensions:
            for choice in dim.choices:
                records.append(
                    {
                        **choice.to_dict(),
                        "dimension_slug": dim.slug,
                        "collection_id": i,
                    }
                )
    # This needs to change to support checkboxes
    df_choices = pd.DataFrame(records).astype("string")

    # Get column names of fields from choice objects
    cols_choices = [col for col in df_choices.columns if col not in ["slug", "collection_id", "dimension_slug"]]

    # For each choice slug, assign an ID (choice_slug_id) that identifies that "slug flavour". E.g. if a slug has different names (or descriptions) across explorers, each "flavour" will have a different ID. This will be useful later to identify conflicts & rename slugs.
    df_choices["choice_slug_id"] = (
        df_choices.groupby(["dimension_slug", "slug"], group_keys=False)
        .apply(
            lambda g: pd.Series(
                pd.factorize(pd.Series(zip(*[g[c] for c in cols_choices])))[0],
                index=g.index,
            ),
            include_groups=False,
        )
        .astype("string")
    )
    # Mark choice slugs as "in conflict": A choice slug maps to different names (or descriptions) across explorers
    df_choices["in_conflict"] = (
        df_choices.groupby(["dimension_slug", "slug"], as_index=False)["choice_slug_id"].transform("nunique").ne(1)
    )

    # Mark choices as duplicates: Same choice properties for a given dimension
    df_choices["duplicate"] = df_choices.duplicated(subset=cols_choices + ["slug", "dimension_slug"])

    # Drop duplicates, except those that are in conflict
    df_choices = df_choices.loc[~df_choices["duplicate"] | df_choices["in_conflict"]]

    # Rename slugs for choices that are duplicates. 'slug' for final slugs, 'slug_original' keeps the original slug
    df_choices.loc[:, "slug_original"] = df_choices.loc[:, "slug"].copy()
    mask = df_choices["in_conflict"]
    df_choices.loc[mask, "slug"] = df_choices.loc[mask, "slug"] + "__" + df_choices.loc[mask, "choice_slug_id"]

    return df_choices, cols_choices
