"""Logic and code to combine multiple collections (MDIMs or Explorers) into a single one.

Additional: combine dimensions (using raw dictionaries)
"""

from copy import deepcopy
from typing import Any, Dict, List, Mapping, Set, Tuple, TypeVar, overload

import pandas as pd
from structlog import get_logger

from etl.collection.core.utils import create_collection_from_config
from etl.collection.explorer import Explorer
from etl.collection.model.core import Collection
from etl.collection.model.dimension import Dimension, DimensionChoice
from etl.collection.utils import records_to_dictionary

log = get_logger()

COLLECTION_SLUG = "collection__slug"
COLLECTION_TITLE = "Collection"

# Define type variables to use in overloads
T = TypeVar("T", bound=Collection)
E = TypeVar("E", bound=Explorer)


# COMBINE DIMENSIONS
def combine_config_dimensions(
    config_dimensions: List[Dict[str, Any]],
    config_dimensions_yaml: List[Dict[str, Any]],
    choices_top: bool = False,
    dimensions_top: bool = False,
):
    """Combine the dimension configuration from the YAML file with the one generated programmatically.

    There are various strategies that we could follow here, but currently:

    - We consider the union of config_dimensions (returned by expander.build_dimensions) nad config_dimensions_yaml.
    - These are kept as-is, unless they are in the YML config, in which case they are overwritten.

    Other possible strategies:

    - We could do the reverse, and only consider the fields from config_dimensions_yaml. I'm personally unsure when this could be valuable.


    Arguments
    ---------
    config_dimensions: List[Dict[str, Any]]
        Generated by expander.build_dimensions.
    config_dimensions_yaml:  List[Dict[str, Any]]
        From the YAML file.
    choices_top: bool
        Set to True to place the choices from `config_dimensions` first.
    dimensions_top: bool
        Set to True to place the dimensions from `config_dimensions` first.

    TODO:

        - I think we need to add more checks to ensure that there is nothing weird being produced here.
    """

    config_dimensions_combined = deepcopy(config_dimensions)
    dims_overwrite = records_to_dictionary(config_dimensions_yaml, "slug")

    # Overwrite dimensions
    for dim in config_dimensions_combined:
        slug_dim = dim["slug"]
        if slug_dim in dims_overwrite:
            # Get dimension data to overwrite, remove it from dictionary
            dim_overwrite = dims_overwrite.pop(slug_dim)

            # Overwrite dimension name
            dim["name"] = dim_overwrite.get("name", dim["name"])

            # Overwrite presentation
            if "presentation" in dim_overwrite:
                dim["presentation"] = dim_overwrite["presentation"]

            # Overwrite description
            if "description" in dim_overwrite:
                dim["description"] = dim_overwrite["description"]

            # Overwrite choices
            if "choices" in dim_overwrite:
                choices_overwrite = records_to_dictionary(
                    dim_overwrite["choices"],
                    "slug",
                )
                assert (
                    "choices" in dim
                ), f"Choices not found in dimension: {dim}! This is rare, please report this issue!"
                for choice in dim["choices"]:
                    slug_choice = choice["slug"]
                    if slug_choice in choices_overwrite:
                        # Get dimension data to overwrite, remove it from dictionary
                        choice_overwrite = choices_overwrite.pop(slug_choice)

                        # Overwrite choice name
                        choice["name"] = choice_overwrite.get("name", dim["name"])
                        # Overwrite choice description
                        description = choice_overwrite.get("description", choice.get("description"))
                        if description is not None:
                            choice["description"] = description
                        # Overwrite group
                        group = choice_overwrite.get("group")
                        if group is not None:
                            choice["group"] = group

                # Handle choices from YAML not present in config_dimensions
                if choices_overwrite:
                    missing_choices = []
                    for slug, values in choices_overwrite.items():
                        choice = {"slug": slug, **values}
                        missing_choices.append(choice)

                    if choices_top:
                        dim["choices"] += missing_choices
                    else:
                        dim["choices"] = missing_choices + dim["choices"]

                # Sort choices based on how these appear in the YAML file (only if dimensions_top is False)
                if not choices_top:
                    dim["choices"] = _order(dim_overwrite["choices"], dim["choices"])

    # Handle dimensions from YAML not present in config_dimensions
    if dims_overwrite:
        missing_dims = []
        for slug, values in dims_overwrite.items():
            dim = {"slug": slug, **values}
            missing_dims.append(dim)

        if dimensions_top:
            config_dimensions_combined += missing_dims
        else:
            config_dimensions_combined = missing_dims + config_dimensions_combined

    # Sort dimensions based on how these appear in the YAML file (only if dimensions_top is False)
    if not dimensions_top:
        config_dimensions_combined = _order(config_dimensions_yaml, config_dimensions_combined)

    return config_dimensions_combined


def _order(config_yaml, config_combined):
    # Build score
    score = {record["slug"]: i for i, record in enumerate(config_yaml)}
    # Split: those that need ordering, those that don't
    config_sort = [record for record in config_combined if record["slug"] in score]
    config_others = [record for record in config_combined if record["slug"] not in score]

    # Order if applicable
    config_sort = sorted(
        config_sort,
        key=lambda x: score.get(x["slug"], 100),
    )

    return config_sort + config_others


@overload
def combine_collections(
    collections: List[E],
    collection_name: str | None = None,
    catalog_path: str | None = None,
    config: dict[str, Any] | None = None,
    dependencies: Set[str] | None = None,
    force_collection_dimension: bool = False,
    collection_dimension_name: str | None = None,
    collection_dimension_slug: str | None = None,
    collection_choices_names: List[str] | None = None,
    is_explorer: bool | None = None,
) -> E: ...


@overload
def combine_collections(
    collections: List[T],
    collection_name: str | None = None,
    catalog_path: str | None = None,
    config: dict[str, Any] | None = None,
    dependencies: Set[str] | None = None,
    force_collection_dimension: bool = False,
    collection_dimension_name: str | None = None,
    collection_dimension_slug: str | None = None,
    collection_choices_names: List[str] | None = None,
    is_explorer: bool | None = None,
) -> T: ...


# COMBINE COLLECTIONS
def combine_collections(
    collections: List[Collection] | List[Explorer],
    collection_name: str | None = None,
    catalog_path: str | None = None,
    config: dict[str, Any] | None = None,
    dependencies: Set[str] | None = None,
    force_collection_dimension: bool = False,
    collection_dimension_name: str | None = None,
    collection_dimension_slug: str | None = None,
    collection_choices_names: List[str] | None = None,
    is_explorer: bool | None = None,
) -> Collection | Explorer:
    """Combine multiple collections (MDIMs or Explorers) into a single one.

    This function serves as a unified interface to combine either Explorers
    or MDIMs (Collections), abstracting the common logic between the two.

    Args:
        collections:
            List of collections (either all MDIMs or all Explorers) to combine
        collection_name:
            Name of the resulting combined collection. This is used to define the `catalog_path` of the resulting combined collection (re-uses the catalog_path of the first collection, and replaced its short_name with the `collection_name`). Alternatively, you can use `catalog_path` to force a specific path.
        catalog_path:
            Force specific catalog path, regardless of `collection_name`.
        config:
            Configuration for the combined collection
        dependencies:
            Set of dependencies for the combined collection
        force_collection_dimension:
            If True, adds a dimension to identify the source collection even if there are no duplicate views
        collection_dimension_slug:
            Slug for the dimension that identifies the source collection. If None, defaults to "collection__slug".
        collection_dimension_name:
            Name for the dimension that identifies the source collection. If None, defaults to "Collection".
        collection_choices_names:
            Names for the choices in the source dimension (should match the length of collections)
        is_explorer:
            Force the result to be an Explorer (True) or MDIM (False). If None (default), inferred from the input collections.

    Returns:
        A combined Collection or Explorer, matching the type of the input collections

    Notes:
        - All collections must have the same dimensions structure (slug, name, etc.)
        - Choice conflicts are resolved by renaming conflicting choices
        - If duplicate views exist, a source dimension is automatically added
    """
    # Check that there are at least 2 collections to combine
    assert len(collections) > 0, "No collections to combine."
    assert len(collections) > 1, "At least two collections should be provided."

    # Check that either collection_name or catalog_path is provided
    if collection_name is None and catalog_path is None:
        raise ValueError("Either collection_name or catalog_path must be provided.")

    # Determine collection type if not specified
    if is_explorer is None:
        is_explorer = all(isinstance(c, Explorer) for c in collections)
        if not (is_explorer or all(not isinstance(c, Explorer) for c in collections)):
            raise ValueError("All collections must be of the same type (either all Explorers or all Collections)")

    # Set appropriate default dimension name based on collection type
    if collection_dimension_name is None:
        collection_dimension_name = COLLECTION_TITLE
    if collection_dimension_slug is None:
        collection_dimension_slug = COLLECTION_SLUG

    # Check that all collections have the same dimensions structure
    collection_dims = None
    for collection in collections:
        dimensions_flatten = [
            {k: v for k, v in dim.to_dict().items() if k != "choices"} for dim in collection.dimensions
        ]
        if collection_dims is None:
            collection_dims = dimensions_flatten
        else:
            assert (
                collection_dims == dimensions_flatten
            ), "Dimensions are not the same across collections. Please review that dimensions are listed in the same order, have the same slugs, names, description, etc."

    # Check for checkbox dimensions in the first collection
    # TODO: Implement support for checkboxes when merging
    for dim in collections[0].dimensions:
        if dim.ui_type == "checkbox" and is_explorer:
            raise NotImplementedError("Checkbox dimensions are not supported yet for Explorers.")

    # Detect duplicate views + save dependencies
    seen_dims = set()
    has_duplicate_views = False
    dependencies_combined = set()
    for collection in collections:
        # duplicate views within a collection
        collection.check_duplicate_views()
        # duplicate views across collections
        for view in collection.views:
            dims = tuple(view.dimensions.items())
            if dims in seen_dims:
                has_duplicate_views = True
                break
            seen_dims.add(dims)

        # Save dependencies from each collection
        dependencies_combined |= collection.dependencies

    # Add collection dimension if needed
    if has_duplicate_views or force_collection_dimension:
        for i, collection in enumerate(collections):
            if collection_choices_names is not None:
                assert len(collection_choices_names) == len(
                    collections
                ), "Length of collection_choices_names must match the number of collections"
                choice_name = collection_choices_names[i]
            else:
                choice_name = collection.title.get("title", collection.short_name)

            dimension_collection = Dimension(
                slug=collection_dimension_slug,
                name=collection_dimension_name,
                choices=[
                    DimensionChoice(slug=collection.short_name, name=choice_name),
                ],
            )
            collection.dimensions = [dimension_collection] + collection.dimensions
            for v in collection.views:
                v.dimensions[collection_dimension_slug] = collection.short_name

    # Create dictionary with collections for tracking
    collections_by_id = {str(i): deepcopy(collection) for i, collection in enumerate(collections)}

    # Build dataframe with all choices
    df_choices, cols_choices = _build_df_choices(collections_by_id)

    # Combine dimensions (use first collection as template)
    dimensions = _combine_dimensions(
        df_choices=df_choices,
        cols_choices=cols_choices,
        collection=collections[0].copy(),
    )

    # Track modifications (useful later for views)
    choice_slug_changes = _extract_choice_slug_changes(df_choices)

    # Update views based on changes to choice slugs
    collections_by_id = _update_choice_slugs_in_views(choice_slug_changes, collections_by_id)

    # Collect all views
    views = []
    for _, collection in collections_by_id.items():
        views.extend(collection.views)

    # Create catalog path
    if isinstance(catalog_path, str):
        assert (
            "#" in catalog_path
        ), "Catalog path must contain a '#' to separate the base path from the collection name."
        catalog_path_new = catalog_path
        collection_name_new = catalog_path_new.split("#")[-1]
    else:
        assert isinstance(collections[0].catalog_path, str), "Catalog path is not set. Please set it before saving."
        assert collection_name is not None, "Collection name must be provided if catalog_path is not set."
        catalog_path_new = collections[0].catalog_path.split("#")[0] + "#" + collection_name
        collection_name_new = collection_name

    # Ensure config has minimal required fields
    if config is None:
        cconfig = {}
    else:
        cconfig = deepcopy(config)

    # Make sure there is title and default_selection. If not given, use default values.
    default_title = {
        "title": f"Combined Collection: {collection_name_new}",
        "title_variant": "Use a YAML to define these attributes",
    }
    if not is_explorer:
        if "title" not in cconfig:
            cconfig["title"] = default_title
        else:
            cconfig["title"] = {**default_title, **cconfig["title"]}
        if "default_selection" not in cconfig:
            cconfig["default_selection"] = collections[0].default_selection
    else:
        if "config" not in cconfig:
            cconfig["config"] = {}
        if "explorerTitle" not in cconfig["config"]:
            cconfig["config"]["explorerTitle"] = default_title["title"]
        if "explorerSubtitle" not in cconfig["config"]:
            cconfig["config"]["explorerSubtitle"] = default_title["title_variant"]

    # Set dimensions and views
    # cconfig["dimensions"] = dimensions
    cconfig["dimensions"] = combine_config_dimensions(
        [d.to_dict() for d in dimensions],
        cconfig.get("dimensions", []),
    )
    cconfig["views"] = views

    # Create the combined collection
    combined = create_collection_from_config(
        config=cconfig,
        dependencies=dependencies if dependencies is not None else set(),
        catalog_path=catalog_path_new,
        validate_schema=True if not is_explorer else False,
        explorer=is_explorer,
        dependencies_combined=dependencies_combined,
    )

    # Log any conflicts that were resolved
    df_conflict = df_choices.loc[df_choices["in_conflict"]]
    if not df_conflict.empty:
        log.warning("Choice slug conflicts resolved")
        for (dimension_slug, choice_slug), group in df_conflict.groupby(["dimension_slug", "slug_original"]):
            log.warning(f"(dimension={dimension_slug}, choice={choice_slug})")
            for _, subgroup in group.groupby("choice_slug_id"):
                collection_ids = subgroup["collection_id"].unique().tolist()
                collection_names = [collections_by_id[i].short_name for i in collection_ids]
                record = subgroup[cols_choices].drop_duplicates().to_dict("records")
                assert len(record) == 1, "Unexpected, please report!"
                log.warning(f" Collections {collection_names} map to {record[0]}")

    return combined


def _extract_choice_slug_changes(df_choices) -> Dict[str, Any]:
    # Track modifications (useful later for views)
    slug_changes = (
        df_choices.loc[df_choices["in_conflict"]]
        .groupby(["collection_id", "dimension_slug"])
        .apply(lambda x: dict(zip(x["slug_original"], x["slug"])), include_groups=False)
        .unstack("collection_id")
        .to_dict()
    )

    return slug_changes


def _combine_dimensions(
    df_choices: pd.DataFrame, cols_choices: List[str], collection: Explorer | Collection
) -> List[Dimension]:
    """Combine dimensions from different explorers"""
    # Dimension bucket
    dimensions = collection.dimensions.copy()

    # Drop duplicates
    df_choices = df_choices.drop_duplicates(subset=cols_choices + ["slug", "dimension_slug"])

    # Iterate over each dimension and update the list of choices
    for dimension in dimensions:
        df_dim_choices = df_choices.loc[
            df_choices["dimension_slug"] == dimension.slug, cols_choices + ["slug"]
        ].drop_duplicates()

        assert (
            len(df_dim_choices) == df_dim_choices["slug"].nunique()
        ), f"Duplicate slugs in dimension {dimension.slug} choices."

        # Raw choices
        choices = df_dim_choices.to_dict("records")

        # Build choices
        dimension.choices = [DimensionChoice.from_dict(c) for c in choices]

    return dimensions


def _update_choice_slugs_in_views(choice_slug_changes, collection_by_id) -> Mapping[str, Collection | Explorer]:
    """Access each explorer, and update choice slugs in views"""
    for collection_id, change in choice_slug_changes.items():
        # Get collection
        collection = collection_by_id[collection_id]

        # Get views as dataframe for easy processing
        df_views_dimensions = pd.DataFrame([view.dimensions for view in collection.views])

        # FUTURE: this needs to change in order to support checkboxes
        df_views_dimensions = df_views_dimensions.astype("string")

        # Process views
        df_views_dimensions = df_views_dimensions.replace(change)

        # Bring back views to collections
        views_dimensions = df_views_dimensions.to_dict("records")
        for view, view_dimensions in zip(collection.views, views_dimensions):
            # cast keys to str to satisfy type requirements
            view.dimensions = {str(key): value for key, value in view_dimensions.items()}
    return collection_by_id


def _build_df_choices(collections_by_id: Mapping[str, Collection | Explorer]) -> Tuple[pd.DataFrame, List[str]]:
    # Collect all choices in a dataframe: choice_slug, choice_name, ..., collection_id, dimension_slug.
    records = []
    for i, explorer in collections_by_id.items():
        for dim in explorer.dimensions:
            for choice in dim.choices:
                records.append(
                    {
                        **choice.to_dict(),
                        "dimension_slug": dim.slug,
                        "collection_id": i,
                    }
                )
    # This needs to change to support checkboxes
    df_choices = pd.DataFrame(records).astype("string")

    # Get column names of fields from choice objects
    cols_choices = [col for col in df_choices.columns if col not in ["slug", "collection_id", "dimension_slug"]]

    # For each choice slug, assign an ID (choice_slug_id) that identifies that "slug flavour". E.g. if a slug has different names (or descriptions) across explorers, each "flavour" will have a different ID. This will be useful later to identify conflicts & rename slugs.
    df_choices["choice_slug_id"] = (
        df_choices.groupby(["dimension_slug", "slug"], group_keys=False)
        .apply(
            lambda g: pd.Series(
                pd.factorize(pd.Series(zip(*[g[c] for c in cols_choices])))[0],
                index=g.index,
            ),
            include_groups=False,
        )
        .astype("string")
    )
    # Mark choice slugs as "in conflict": A choice slug maps to different names (or descriptions) across explorers
    df_choices["in_conflict"] = (
        df_choices.groupby(["dimension_slug", "slug"], as_index=False)["choice_slug_id"].transform("nunique").ne(1)
    )

    # Mark choices as duplicates: Same choice properties for a given dimension
    df_choices["duplicate"] = df_choices.duplicated(subset=cols_choices + ["slug", "dimension_slug"])

    # Drop duplicates, except those that are in conflict
    df_choices = df_choices.loc[~df_choices["duplicate"] | df_choices["in_conflict"]]

    # Rename slugs for choices that are duplicates. 'slug' for final slugs, 'slug_original' keeps the original slug
    df_choices.loc[:, "slug_original"] = df_choices.loc[:, "slug"].copy()
    mask = df_choices["in_conflict"]
    df_choices.loc[mask, "slug"] = df_choices.loc[mask, "slug"] + "__" + df_choices.loc[mask, "choice_slug_id"]

    return df_choices, cols_choices
