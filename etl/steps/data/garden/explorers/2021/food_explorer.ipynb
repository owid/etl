{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fe55dd-d6a8-4b04-b4de-40a93d325bfd",
   "metadata": {},
   "source": [
    "# Food Explorer\n",
    "Produced using garden-level FAOstat datasets. \n",
    "\n",
    "So far the following datasets have been processed:\n",
    "\n",
    "- [x] QCL\n",
    "- [x] FBSC (FBS, FBSH)\n",
    "\n",
    "\n",
    "We process both datasets in parallel, until the _Final Processing_ section, where we actually merge the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181739bb-0909-465a-afdd-0efc47e2d09a",
   "metadata": {},
   "source": [
    "## 0. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35e10c-ecda-4c86-8783-56992f6f855e",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dest_dir = \"/tmp/food_explorer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35dbb4-850f-491d-b071-7eab7fe64253",
   "metadata": {},
   "source": [
    "## 1. Imports & paths\n",
    "Import the required libraries and define paths to load files (including data files and standardisation mappings for item and element names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a7abe-6dd5-48c7-bde2-99cda0e821e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from owid import catalog\n",
    "from etl.paths import BASE_DIR, DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e2966-1ad0-4a6d-9673-c4ef8c94b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE = BASE_DIR / \"etl/steps/data/garden/explorers/2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51901d1-fe72-4cc0-aef2-ea339140214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET_QCL = DATA_DIR / \"garden/faostat/2021-03-18/faostat_qcl\"\n",
    "PATH_DATASET_FBSC = DATA_DIR / \"garden/faostat/2021-04-09/faostat_fbsc\"\n",
    "PATH_DATASET_POPULATION = DATA_DIR / \"garden/owid/latest/key_indicators\"\n",
    "PATH_DATASET_POPULATION_GAPMINDER = (\n",
    "    DATA_DIR / \"open_numbers/open_numbers/latest/gapminder__systema_globalis\"\n",
    ")  # add\n",
    "\n",
    "PATH_MAP_ITEM = HERE / \"food_explorer.items.std.csv\"\n",
    "PATH_MAP_ELEM = HERE / \"food_explorer.elements.std.csv\"\n",
    "PATH_REGIONS = HERE / \"food_explorer.regions.json\"\n",
    "PATH_OUTLIERS = HERE / \"food_explorer.outliers.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2966f-c5e6-4039-825b-824fb430724c",
   "metadata": {},
   "source": [
    "## 2. Load garden dataset\n",
    "In this step we load the required datasets from Garden: QCL and FBSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5769436-dfed-466b-9108-d534c59b7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qcl_garden = catalog.Dataset(PATH_DATASET_QCL)\n",
    "fbsc_garden = catalog.Dataset(PATH_DATASET_FBSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ca7ec-05f0-42b3-ab4b-2bced4c5b344",
   "metadata": {},
   "source": [
    "We obtain table `bulk` from the dataset, which contains the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc462f-3dec-46c7-8d99-e89242fc38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk data and items metadata\n",
    "qcl_bulk = qcl_garden[\"bulk\"]\n",
    "fbsc_bulk = fbsc_garden[\"bulk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a9471-8b2b-46c8-8152-18e0c035446b",
   "metadata": {},
   "source": [
    "In the following step we discard column `variable_name`, which although useful for its clarity we don't actually need it in this process. Also, we reset the index as this will be needed in following operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a7d5d-48ec-4700-8795-e360245252c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QCL\n",
    "qcl_bulk = qcl_bulk.reset_index()\n",
    "qcl_bulk = qcl_bulk.drop(columns=[\"variable_name\"])\n",
    "# FBSC\n",
    "fbsc_bulk = fbsc_bulk.reset_index()\n",
    "fbsc_bulk = fbsc_bulk.drop(columns=[\"variable_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c96121-c608-4ac0-9b55-ad47c1f2eb3a",
   "metadata": {},
   "source": [
    "Brief overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88461fe-c796-4cd2-8007-dadb141e01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QCL\n",
    "print(qcl_bulk.shape)\n",
    "qcl_bulk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f2ffb-ea32-4628-b070-affa5ea29a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FBSC\n",
    "print(fbsc_bulk.shape)\n",
    "fbsc_bulk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a09781-9b1f-40c6-b869-566a16185e7c",
   "metadata": {},
   "source": [
    "### Group some items\n",
    "We know from Garden process to generate the FBSC dataset, that there are some items that \"changed\" its ID from one dataset to another:\n",
    "\n",
    "- `2556 Groundnuts (Shelled Eq)` --> `2552 Groundnuts`\n",
    "- `2805 Rice (Milled Equivalent)` --> `2807 Rice and products`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec594e6-8bce-462a-bc6b-f0f619cc5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_item_codes(df, ids_old, ids_new, assign_to_old=False):\n",
    "    # Check\n",
    "    msk = df[\"item_code\"].isin(ids_old + ids_new)\n",
    "    x = df[msk].groupby(\"item_code\").agg({\"year\": [\"min\", \"max\"]})\n",
    "    for id_old, id_new in zip(ids_old, ids_new):\n",
    "        assert x.loc[id_new, (\"year\", \"min\")] > x.loc[id_old, (\"year\", \"max\")]\n",
    "    # Replace\n",
    "    if isinstance(assign_to_old, list):\n",
    "        id_map = dict(\n",
    "            (n, o) if f else (o, n) for o, n, f in zip(ids_old, ids_new, assign_to_old)\n",
    "        )\n",
    "    elif assign_to_old:\n",
    "        id_map = dict(zip(ids_new, ids_old))\n",
    "    else:\n",
    "        id_map = dict(zip(ids_old, ids_new))\n",
    "    print(id_map)\n",
    "    df[\"item_code\"] = df[\"item_code\"].replace(id_map).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467850f-4088-4d09-a467-a9860ddb81fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbsc_bulk = group_item_codes(\n",
    "    fbsc_bulk, ids_old=[2556, 2805], ids_new=[2552, 2807], assign_to_old=[True, True]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c3fc8-c58f-4c6b-8024-7ee6a1a22e18",
   "metadata": {},
   "source": [
    "## 3. Select flags\n",
    "There are cases where we have more than just one entry for a `country`, `item_code`, `element_code` and `year`. This is due to the fact that there are multiple ways of reporting the data. All these different methodologies are identified by the field `flag`, which tells us how a data point was obtained (see table below). This is given by FAOstat.\n",
    "\n",
    "|flag   |description                                                                        |\n",
    "|-------|-----------------------------------------------------------------------------------|\n",
    "|`*`      |       Unofficial figure                                                           |\n",
    "|`NaN`    | Official data                                                                     |\n",
    "|`A`      |       Aggregate; may include official; semi-official; estimated or calculated data|\n",
    "|`F`      |       FAO estimate                                                                |\n",
    "|`Fc`     |      Calculated data                                                              |\n",
    "|`Im`     |      FAO data based on imputation methodology                                     |\n",
    "|`M`      |       Data not available                                                          |\n",
    "|`S`      |       Standardised                                                                |\n",
    "|`SD`     |       Statistical Discrepancy                                                     |\n",
    "|`R`      |       Estimated data using trading partners database                              |\n",
    "\n",
    "\n",
    "The following cell examines how many datapoints would be removed if we did _flag-prioritisation_. As per the output, we see that we would eliminate 30,688 rows (~1% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62537795-b1bd-4495-b221-3a20a3c575b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_flags_1(df):\n",
    "    i_og = df.index.tolist()\n",
    "    i_ne = df.drop_duplicates(\n",
    "        subset=[\"country\", \"item_code\", \"element_code\", \"year\"]\n",
    "    ).index.tolist()\n",
    "    print(\n",
    "        f\"Number of datapoints: {len(i_og)}\\nNumber of datapoints (after dropping duplicates): {len(i_ne)}\\nTotal datapoints removed: {len(i_og)-len(i_ne)}\"\n",
    "    )\n",
    "    check_flags_2(df, i_og, i_ne)\n",
    "\n",
    "\n",
    "def check_flags_2(df, i_og, i_ne):\n",
    "    \"\"\"Prints `[number of datapoints eliminated], True`\"\"\"\n",
    "    df = df.set_index([\"country\", \"item_code\", \"element_code\", \"year\"])\n",
    "    dups = df.index.duplicated()\n",
    "    print(f\"{dups.sum()}, {len(i_ne) == len(i_og)-dups.sum()}\")\n",
    "    # dups = qcl_bulk.index.duplicated(keep=False)\n",
    "    df = df.reset_index()\n",
    "\n",
    "\n",
    "check_flags_1(qcl_bulk)\n",
    "print()\n",
    "check_flags_1(fbsc_bulk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcd2b9-d199-45a9-a87c-2a0930209b05",
   "metadata": {},
   "source": [
    "### Flag prioritzation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e764b-fb83-45c0-a604-9ff31c12a283",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this step we define a flag prioritisation rank, which allows us to discard duplicate entries based on which flag we \"prefer\". We do this by assigning a weight to each datapoint based on their `flag` value (the higher, the more prioritised it is). On top of flag prioritisation, we always prefer non-`NaN` values regardless of their associated `flag` value (we assign weight -1 to this datapoints). The weighting was shared and discussed with authors. \n",
    "\n",
    "The weight is added to the dataframe as a new column `flag_priority`.\n",
    "\n",
    "#### Example 1\n",
    "\n",
    "    country, year, product, value, flag \n",
    "    Afghanistan, 1993, Apple, 100, F\n",
    "    Afghanistan, 1993, Apple, 120, A\n",
    "\n",
    "We would choose first row, with flag F.\n",
    "\n",
    "#### Example 2:\n",
    "\n",
    "    country, year, product, value, flag \n",
    "    Afghanistan, 1993, Apple, NaN, F\n",
    "    Afghanistan, 1993, Apple, 120, A\n",
    "\n",
    "We would choose second row, as first row is `NaN`.\n",
    "\n",
    "\n",
    "In the following cell we filter rows based on `FLAG_PRIORITIES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da2eda-1fca-47d0-9bd3-7da00c99d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flag priority (add to df) More info at https://www.fao.org/faostat/en/#definitions\n",
    "FLAG_PRIORITIES = {\n",
    "    \"M\": 0,  # Data not available\n",
    "    \"SD\": 10,  # Statistical Discrepancy\n",
    "    \"*\": 20,  # Unofficial figure\n",
    "    \"R\": 30,  # Estimated data using trading partners database\n",
    "    \"Fc\": 40,  # Calculated data\n",
    "    \"S\": 60,  # Standardized data\n",
    "    \"A\": 70,  # Aggregate; may include official; semi-official; estimated or calculated data\n",
    "    \"Im\": 80,  # FAO data based on imputation methodology\n",
    "    \"F\": 90,  # FAO estimate\n",
    "    np.nan: 100,  # Official data\n",
    "}\n",
    "\n",
    "\n",
    "def filter_by_flag_priority(df):\n",
    "    # Add flag priority column\n",
    "    df.loc[:, \"flag_priority\"] = df.flag.replace(FLAG_PRIORITIES).tolist()\n",
    "    df.loc[df.value.isna(), \"flag_priority\"] = -1\n",
    "    # Remove duplicates based on flag value\n",
    "    df = df.sort_values(\"flag_priority\")\n",
    "    df = df.drop_duplicates(\n",
    "        subset=[\"country\", \"item_code\", \"element_code\", \"year\"], keep=\"last\"\n",
    "    )\n",
    "    return df.drop(columns=[\"flag_priority\", \"flag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb64782-3fe5-4ac4-a59c-013f1221ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QCL\n",
    "qcl_bulk = filter_by_flag_priority(qcl_bulk)\n",
    "print(qcl_bulk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c748ef-2016-4738-9304-cbffd1c6d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FBSC\n",
    "fbsc_bulk = filter_by_flag_priority(fbsc_bulk)\n",
    "print(fbsc_bulk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6f901-1b02-4faa-bb9a-a016b2d45b2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Element Overview\n",
    "This serves as an initial check on the meaning of `element_code` values. In particular, we note that each `element_code` value corresponds to a unique pair of _element name_  and _element unit_. Note, for instance, that _element_name_ \"production\" can come in different flavours (i.e. units): \"production -- tones\" and \"production -- 1000 No\".\n",
    "\n",
    "Based on the number of occurrences of each element_code, we may want to keep only those that rank high.\n",
    "\n",
    "**Note: This step uses file `PATH_MAP_ELEM`, which is a file that was generated using the code in a later cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61681c4f-0585-48c6-85d1-1adf9586d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where do each element appear?\n",
    "def get_stats_elements(df):\n",
    "    res = df.reset_index().groupby(\"element_code\")[\"item_code\"].nunique()\n",
    "    df_elem = pd.read_csv(PATH_MAP_ELEM, index_col=\"code\")\n",
    "    elem_map = (\n",
    "        df_elem[\"name\"] + \" -- \" + df_elem[\"unit\"] + \" -- \" + df_elem.index.astype(str)\n",
    "    )\n",
    "    res = res.rename(index=elem_map.to_dict()).sort_values(ascending=False)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5e095-b849-4fac-b731-bd78a6e16952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QCL\n",
    "get_stats_elements(qcl_bulk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd067c44-bc63-4d05-af0e-0fc6f7ae3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FBSC\n",
    "get_stats_elements(fbsc_bulk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984267f3-77e3-4076-8079-b8c459bcde3e",
   "metadata": {},
   "source": [
    "## 5. Reshape dataset\n",
    "This step is simple and brief. It attempts to pivot the dataset in order to have three identifying columns (i.e. \"keys\") and several \"value\" columns based on the `element_code` and `Value` columns.\n",
    "\n",
    "This format is more Grapher/Explorer friendly, as it clearly divides the dataset columns into: Entities, year, [Values]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78ddb6-1100-4658-8043-44964e869a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_df(df):\n",
    "    df = df.reset_index()\n",
    "    df = df.pivot(\n",
    "        index=[\"country\", \"item_code\", \"year\"], columns=\"element_code\", values=\"value\"\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbe8b1-9f74-4af1-ad37-1df1e262edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QCL\n",
    "qcl_bulk = reshape_df(qcl_bulk)\n",
    "# FBSC\n",
    "fbsc_bulk = reshape_df(fbsc_bulk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5471b-a00e-4996-894c-6d8709bb3713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"QCL:\", qcl_bulk.shape)\n",
    "print(\"FBSC:\", fbsc_bulk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d394c-5de2-4412-8277-91c9a5857ce2",
   "metadata": {},
   "source": [
    "## 6. Standardise Element and Item names (OPTIONAL)\n",
    "In the following cells we obtain tables with the code, current name and number of occurrences of all the Items and Elements present in our dataset.\n",
    "\n",
    "Based on this tables, Hannah (or another researcher), will revisit these and:\n",
    "- Select those Items and Elements that we are interested in.\n",
    "- Standardise naming proposals of Items and Elements.\n",
    "\n",
    "Notes:\n",
    "- We obtain the number of occurrences as this can assist the researcher in prioritising Items or Elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ccc40-ea1e-44b5-ae3f-f81aba420cbd",
   "metadata": {},
   "source": [
    "### Elements\n",
    "Here we obtain a table with the current namings for Elements (plus other variables). Note that we also propagate the unit names, as these may also be standardised (or even changed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2dcf6d-453b-4d3c-a5e4-87e3620c5908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table from dataset containing Element information\n",
    "qcl_elem = qcl_garden[\"meta_qcl_element\"]\n",
    "fbsc_elem = fbsc_garden[\"meta_fbs_element\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f69abc-a5ea-4ac4-a175-7dbdc4543f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elements_to_standardize(df, df_elem):\n",
    "    # Obtain number of occurrences for each element_code (each column is an element)\n",
    "    elements = pd.DataFrame(df.notna().sum()).reset_index()\n",
    "    elements = elements.sort_values(0, ascending=False)\n",
    "    # Add names and unit info to the table\n",
    "    elements = elements.merge(\n",
    "        df_elem[[\"element\", \"unit\", \"unit_description\"]],\n",
    "        left_on=\"element_code\",\n",
    "        right_index=True,\n",
    "    )\n",
    "    # Rename column names\n",
    "    elements = elements.rename(\n",
    "        columns={\n",
    "            \"element_code\": \"code\",\n",
    "            0: \"number_occurrences\",\n",
    "            \"element\": \"name\",\n",
    "            \"unit\": \"unit\",\n",
    "            \"unit_description\": \"unit_description\",\n",
    "        }\n",
    "    )[[\"code\", \"name\", \"unit\", \"unit_description\", \"number_occurrences\"]]\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c807483-c460-4fd6-a3b7-ff70e4a94b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_qcl = get_elements_to_standardize(qcl_bulk, qcl_elem).assign(dataset=\"QCL\")\n",
    "elements_fbsc = get_elements_to_standardize(fbsc_bulk, fbsc_elem).assign(dataset=\"FBSC\")\n",
    "\n",
    "assert elements_qcl.merge(elements_fbsc, on=\"code\").empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff6741-e898-4ea8-9a50-a3ba84810942",
   "metadata": {},
   "source": [
    "Once the table is obtained, we take a look at it and export it. Note that we use a filename starting with `ign.`, as these are note git-tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dfcee6-ee59-46b8-a176-c69fb3cef5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = pd.concat([elements_qcl, elements_fbsc])\n",
    "elements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4efc5-50ed-4164-bffb-baf9af3fd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elements.to_csv(\"ign.food.elements.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001cfba2-9b59-400c-b2fe-5c694f5a1ff3",
   "metadata": {},
   "source": [
    "### Items\n",
    "Here we obtain a table with the current namings for Items (plus other variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13063f01-bb80-4579-9780-b2051da0b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table from dataset containing Item information\n",
    "qcl_item = qcl_garden[\"meta_qcl_item\"]\n",
    "fbsc_item = fbsc_garden[\"meta_item\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d6357-d609-4f9a-b0a2-692d299fd782",
   "metadata": {},
   "source": [
    "As the following cell shows, this table comes with a multi-index, as codes may actually be referring to \"item_groups\" or \"Items\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fb733-5350-493f-805f-ad3cc0fd083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qcl_item.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cedc32e-9221-4fa2-a0d3-51cb7aaa4a47",
   "metadata": {},
   "source": [
    "Therefore, in the next cell we attempt to flatten code to name mappings.\n",
    "\n",
    "To this end:\n",
    "- We first create two separate dictionaries, mapping `item_group_code --> item_group` and `item_code --> Item`, respectively.\n",
    "- We note, however, that some codes appear both as \"Items\" and \"item_groups\". This might be due to the fact that there are more than one level of items. That is, an Item can \"belong\" to an item_group, which in turn belongs to yet a higher up item_group. Therefore, we remove these codes from the item dictionary so they only appear in the item_group dictionary.\n",
    "- Next, we create a table with all items, their occurrences, whether they are item_groups, and their FAO original namings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbf0fa-9b2c-43da-b3bd-a6e441b89b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_to_standardize(df, df_item):\n",
    "    # Group\n",
    "    map_item_g = dict(\n",
    "        zip(\n",
    "            df_item.index.get_level_values(\"item_group_code\").astype(str),\n",
    "            df_item[\"item_group\"],\n",
    "        )\n",
    "    )\n",
    "    # Item\n",
    "    map_item = dict(\n",
    "        zip(df_item.index.get_level_values(\"item_code\").astype(str), df_item[\"item\"])\n",
    "    )\n",
    "\n",
    "    # Correct\n",
    "    map_item = {k: v for k, v in map_item.items() if k not in map_item_g}\n",
    "\n",
    "    # Load item occurences\n",
    "    items = (\n",
    "        pd.DataFrame(df.reset_index()[\"item_code\"].value_counts())\n",
    "        .reset_index()\n",
    "        .astype(str)\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"index\": \"code\",\n",
    "                \"item_code\": \"number_occurences\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    # Add flag for groups\n",
    "    items[\"type\"] = (\n",
    "        items[\"code\"].isin(map_item_g).apply(lambda x: \"Group\" if x else None)\n",
    "    )\n",
    "    # Add name\n",
    "    map_item_all = {**map_item, **map_item_g}\n",
    "    items[\"name\"] = items.code.replace(map_item_all)\n",
    "    # Order columns\n",
    "    items = items[[\"code\", \"name\", \"type\", \"number_occurences\"]]\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daace449-144b-44fb-9d05-66727b998dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_qcl = get_items_to_standardize(qcl_bulk, qcl_item).assign(dataset=\"QCL\")\n",
    "items_fbsc = get_items_to_standardize(fbsc_bulk, fbsc_item).assign(dataset=\"FBSC\")\n",
    "items = pd.concat([items_qcl, items_fbsc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce92523-1cbf-41f3-96a8-b443656c02c7",
   "metadata": {},
   "source": [
    "Once the table is obtained, we take a look at it and export it. Note that we use a filename starting with `ign.`, as these are note git-tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864333e-d00e-42e3-a977-7d6bc736dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ca032-bc4c-4170-87df-83c0a1563e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items.to_csv(\"ign.food.items.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411b093-9cd8-47a2-b31e-7fcd62f91dcf",
   "metadata": {},
   "source": [
    "## 7. Renaming Items and Elements\n",
    "After the previous step, where we shared files `ign.food.items.csv` and `ign.food.elements.csv` with a researcher, they will review them and add the standardisation namings for all items and elements that we intend to use. Note that if no standardised name is provided, the item or element will be discarded.\n",
    "\n",
    "Their proposals come in two files: `food_explorer.items.std.csv` and `food_explorer.elements.std.csv`. Note that we prefer working with the mapping `\"item/element_code\" ---> \"new standardised item/element name\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa055e6f-78a9-4afe-82e4-e3137a1133c7",
   "metadata": {},
   "source": [
    "### Element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174f1fd-c9e5-4660-8617-4ea6478cb3a5",
   "metadata": {},
   "source": [
    "First of all, we load the standardisation table and remove NaN values (these belong to to-be-discarded elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd878f-47ca-4569-8bb4-f764f1244d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standardised values\n",
    "df = pd.read_csv(PATH_MAP_ELEM, index_col=\"code\")\n",
    "df = df.dropna(subset=[\"name_standardised\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a3f1a-c4e2-43d7-b32f-593323ef703e",
   "metadata": {},
   "source": [
    "If we display the content of the standardisation element file we observe that:\n",
    "- Only some elements are preserved.\n",
    "- There is the column `unit_name_standardised_with_conversion` and `unit_factor`, which provide the new unit and the factor to convert the old one into the new one. \n",
    "- Multiple codes are assigned to the same `name_standardised` and `unit_name_standardised_with_conversion`, which means that we will have to merge them. In particular, element \"Yield\" with unit \"kg/animal\" appears with four different codes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4a477-197a-411d-8d52-d103036cf54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd2763-c2f4-40f1-8eb6-8d4b10f51dfc",
   "metadata": {},
   "source": [
    "We keep columns in data file that belong to the \"elements of interest\" (those with renaming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a980add-06d6-4b8d-96b0-be53678bd3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter elements of interest\n",
    "qcl_bulk = qcl_bulk[[col for col in df.index if col in qcl_bulk.columns]]\n",
    "fbsc_bulk = fbsc_bulk[[col for col in df.index if col in fbsc_bulk.columns]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab47d73-efc9-4aab-b175-86537cb76a2f",
   "metadata": {},
   "source": [
    "We modify the values of some elements, based on the new units and `unit_factor` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30000a-40bd-4a97-a99e-775127cb49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor\n",
    "qcl_bulk = qcl_bulk.multiply(df.loc[qcl_bulk.columns, \"unit_factor\"])\n",
    "fbsc_bulk = fbsc_bulk.multiply(df.loc[fbsc_bulk.columns, \"unit_factor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fee009-0f47-43ed-833a-c33d7507d8f3",
   "metadata": {},
   "source": [
    "Next, we merge codes into single codes:\n",
    "- **Yield**: `5417, 5420, 5424, 5410 ---> 5417` (QCL)\n",
    "- **Animals slaughtered**: `5320, 5321 ---> 5320` (QCL)\n",
    "\n",
    "As previously highlighted, all of them are mapped to the same (name, unit) tupple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45399108-669d-4771-a55e-84af45544cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QCL\n",
    "item_code_merge = {\n",
    "    5417: [5420, 5424, 5410],\n",
    "    5320: [5321],\n",
    "}\n",
    "items_drop = [ii for i in item_code_merge.values() for ii in i]\n",
    "for code_new, codes_old in item_code_merge.items():\n",
    "    for code_old in codes_old:\n",
    "        qcl_bulk[code_new] = qcl_bulk[code_new].fillna(qcl_bulk[code_old])\n",
    "qcl_bulk = qcl_bulk.drop(columns=items_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398da7e-3511-413d-b94d-8bb965972acd",
   "metadata": {},
   "source": [
    "Finally, we rename the column names (so far element_codes) to more prosaic element identifiers (`[element-name]__[unit]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06553ff8-56e3-4c03-8e01-ec36c172c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build element name\n",
    "a = df[\"name_standardised\"].apply(lambda x: x.lower().replace(\" \", \"_\")).astype(str)\n",
    "b = (\n",
    "    df[\"unit_name_standardised_with_conversion\"]\n",
    "    .apply(lambda x: x.lower().replace(\" \", \"_\"))\n",
    "    .astype(str)\n",
    ")\n",
    "df[\"element_name\"] = (a + \"__\" + b).tolist()\n",
    "# Obtain dict element_code -> element name\n",
    "map_elem = df[\"element_name\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c11c6-d286-4c42-9612-105db99d32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change columns names\n",
    "qcl_bulk = qcl_bulk.rename(columns=map_elem)\n",
    "fbsc_bulk = fbsc_bulk.rename(columns=map_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec1808-8715-4473-b129-98434c4eadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataframe with standardised element names\n",
    "qcl_bulk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb1b3e-15ad-4638-b47e-552072672bbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Item\n",
    "We now load the standardisation item table and remove `NaN` values (these belong to to-be-discarded items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b564750-ee04-48d2-85c3-7c0c00c351b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get standardised values\n",
    "df = pd.read_csv(PATH_MAP_ITEM, index_col=\"code\")\n",
    "map_item_std = df.dropna(subset=[\"name_standardised\"])[\"name_standardised\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74daed2-02d1-492d-82de-f7be8265c42f",
   "metadata": {},
   "source": [
    "Briefly display first 10 mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f4012-4b80-41a1-8cf0-d83d887d6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for (k, v) in list(map_item_std.items())[:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378585ca-c87d-41f5-b3d2-432a307c5eb3",
   "metadata": {},
   "source": [
    "Next, we do a simple check of item name uniqueness. Note that we can have multiple codes assigned to the same `name_standardised`, as part of the standardisation process, BUT these should be in different datasets so we don't have any element conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18fe34-5475-4dd6-b285-2e2dd2d3ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show \"fused\" products from QCL and FBSC\n",
    "x = pd.DataFrame.from_dict(map_item_std, orient=\"index\", columns=[\"name\"]).reset_index()\n",
    "x = x.groupby(\"name\").index.unique().apply(list)\n",
    "x = x[x.apply(len) > 1]\n",
    "print(\"There are\", len(x), \"fused products:\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb99ea9-2457-4d22-ae23-52d15f71e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check `code` --> `name_standardised` is unique in each dataset\n",
    "assert (\n",
    "    df.dropna(subset=[\"name_standardised\"])\n",
    "    .reset_index()\n",
    "    .groupby([\"dataset\", \"name_standardised\"])\n",
    "    .code.nunique()\n",
    "    .max()\n",
    "    == 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda3907-d942-4254-8189-58b1096fce16",
   "metadata": {},
   "source": [
    "Next, we filter out items that we are not interested in and add a new column (`product`) with the standardised item names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68a20f-e46f-4aa8-8ba9-4ec010839468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_product_names(df):\n",
    "    df = df.reset_index()\n",
    "    df = df[df[\"item_code\"].isin(map_item_std)]\n",
    "    df.loc[:, \"product\"] = df[\"item_code\"].replace(map_item_std).tolist()\n",
    "    df = df.drop(columns=[\"item_code\"])\n",
    "    # Set back index\n",
    "    df = df.set_index([\"product\", \"country\", \"year\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e522f-7a57-44c3-918a-a519ca9464f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qcl_bulk = standardise_product_names(qcl_bulk)\n",
    "fbsc_bulk = standardise_product_names(fbsc_bulk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade0d82-150b-468b-960a-2aa3e4fedb7e",
   "metadata": {},
   "source": [
    "## 8. Dataset merge\n",
    "Here we add the final processing steps:\n",
    "- Merge datasets `QCL` + `FBSC`\n",
    "- Discard products (former items) that do not contain any value for the \"elements of interest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82898260-9b51-49bd-a23b-8a289abc7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "fe_bulk = pd.merge(qcl_bulk, fbsc_bulk, how=\"outer\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c473b45-e1b6-4f54-9746-2b8ccf0c7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"QCL // shape:\", qcl_bulk.shape, \"/ not-NaN:\", qcl_bulk.notna().sum().sum())\n",
    "print(\"FBSC // shape:\", fbsc_bulk.shape, \"/ not-NaN:\", fbsc_bulk.notna().sum().sum())\n",
    "print(\"FE // shape:\", fe_bulk.shape, \"/ not-NaN:\", fe_bulk.notna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc3d98-4d00-45f3-97a1-c261bbf554b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls (some products dont have any value for the elements of interest)\n",
    "fe_bulk = fe_bulk.dropna(how=\"all\")\n",
    "print(\"FE (after NaN-drop):\", fe_bulk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65eb08-512a-4656-9856-692689944978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fe_bulk.shape)\n",
    "fe_bulk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9654f5-26e9-479d-adc5-4c09635aa138",
   "metadata": {},
   "source": [
    "## 9. Post processing\n",
    "In this section we obtain the metrics for all regions and add per-capita counterparts. So far, we include income groups by the World Bank, continents as defined by OWID and World. The values for these entities are obtained using only data present in the dataset (i.e. some countries may be missing).\n",
    "\n",
    "\n",
    "- Normalize metrics\n",
    "    - Add population column\n",
    "    - Weight columns\n",
    "    - Rename columns\n",
    "- Obtain metrics for regions\n",
    "- Add population column, including regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f21c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe_bulk_orig = fe_bulk.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae636c-79f3-44bc-ad2f-f4f3fe4c4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_bulk = fe_bulk.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1ddf1-9be4-4d57-aadc-56abb723e603",
   "metadata": {},
   "source": [
    "### 9.0 Build population table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca60743-e9d7-4750-aa8e-a0b0c96c31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population dataset\n",
    "indicators = catalog.Dataset(PATH_DATASET_POPULATION)\n",
    "population = indicators[\"population\"][[\"population\"]].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e9e9e-2e93-4981-bf01-8b9f312f8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from gapminder (former countries)\n",
    "# more info: https://github.com/open-numbers/ddf--gapminder--systema_globalis/blob/master/ddf--entities--geo--country.csv\n",
    "gapminder = catalog.Dataset(PATH_DATASET_POPULATION_GAPMINDER)\n",
    "population_gap = (\n",
    "    gapminder[\"total_population_with_projections\"]\n",
    "    .reset_index()\n",
    "    .rename(columns={\"time\": \"year\", \"total_population_with_projections\": \"population\"})\n",
    ")\n",
    "\n",
    "gapminder_country_codes = {\n",
    "    \"ussr\": \"USSR\",\n",
    "    \"cheslo\": \"Czechoslovakia\",\n",
    "    \"yug\": \"Yugoslavia\",\n",
    "    \"eri_a_eth\": \"Eritrea and Ethiopia\",\n",
    "    \"scg\": \"Serbia and Montenegro\",\n",
    "}\n",
    "former_states = list(gapminder_country_codes.values())\n",
    "\n",
    "population_gap = population_gap[population_gap.geo.isin(gapminder_country_codes)]\n",
    "population_gap = population_gap.assign(\n",
    "    country=population_gap.geo.map(gapminder_country_codes)\n",
    ").drop(columns=[\"geo\"])\n",
    "\n",
    "# Filter years (former states only for past interval, not overlapping with current countries)\n",
    "date_window = (\n",
    "    fe_bulk[fe_bulk.country.isin(former_states)]\n",
    "    .groupby(\"country\")\n",
    "    .year.agg([\"min\", \"max\"])\n",
    "    .to_dict(orient=\"index\")\n",
    ")\n",
    "population_ = []\n",
    "for state, dates in date_window.items():\n",
    "    df_ = population_gap[\n",
    "        (population_gap.country == state)\n",
    "        & (population_gap.year >= dates[\"min\"])\n",
    "        & (population_gap.year <= dates[\"max\"])\n",
    "    ]\n",
    "    population_.append(df_)\n",
    "\n",
    "population_gap = pd.concat(population_, ignore_index=True)\n",
    "\n",
    "# Index\n",
    "population_gap = population_gap.set_index([\"country\", \"year\"], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f457b-3bfb-43f4-b459-d8ba0f3f861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no overlapping\n",
    "former_to_current = {\n",
    "    \"USSR\": [\n",
    "        \"Lithuania\",\n",
    "        \"Georgia\",\n",
    "        \"Estonia\",\n",
    "        \"Latvia\",\n",
    "        \"Ukraine\",\n",
    "        \"Moldova\",\n",
    "        \"Kyrgyzstan\",\n",
    "        \"Uzbekistan\",\n",
    "        \"Tajikistan\",\n",
    "        \"Armenia\",\n",
    "        \"Azerbaijan\",\n",
    "        \"Turkmenistan\",\n",
    "        \"Belarus\",\n",
    "        \"Russia\",\n",
    "        \"Kazakhstan\",\n",
    "    ],\n",
    "    \"Yugoslavia\": [\n",
    "        \"Croatia\",\n",
    "        \"Slovenia\",\n",
    "        \"North Macedonia\",\n",
    "        \"Bosnia and Herzegovina\",\n",
    "        \"Serbia\",\n",
    "        \"Montenegro\",\n",
    "    ],\n",
    "    \"Czechoslovakia\": [\"Czechia\", \"Slovakia\"],\n",
    "    \"Eritrea and Ethiopia\": [\"Ethiopia\", \"Eritrea\"],\n",
    "    \"Serbia and Montenegro\": [\"Serbia\", \"Montenegro\"],\n",
    "    \"Sudan (former)\": [\"Sudan\", \"South Sudan\"],\n",
    "}\n",
    "former_states = list(former_to_current.keys())\n",
    "\n",
    "for former, current in former_to_current.items():\n",
    "    msk = fe_bulk.country.isin(current)\n",
    "    current_start = fe_bulk.loc[msk, \"year\"].min()\n",
    "    former_end = fe_bulk.loc[fe_bulk.country == former, \"year\"].max()\n",
    "    assert former_end < current_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ea546-9789-44a6-9d09-f29d1d60d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Sudan (former)\n",
    "msk = population.country.isin([\"South Sudan\", \"Sudan\"]) & (population.year < 2012)\n",
    "pop_sudan = (\n",
    "    population[msk]\n",
    "    .groupby(\"year\", as_index=False)\n",
    "    .population.sum()\n",
    "    .assign(country=\"Sudan (former)\")\n",
    ")\n",
    "population = pd.concat([pop_sudan, population], ignore_index=True)\n",
    "date_window = date_window | {\"Sudan (former)\": {\"min\": 1961, \"max\": 2011}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a6f82-1f1d-4ef9-9225-442673fc5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter current states that did not exist\n",
    "msk = None\n",
    "for former, current in former_to_current.items():\n",
    "    if msk is None:\n",
    "        msk = population.country.isin(former_to_current[former]) & (\n",
    "            population.year <= date_window[former][\"max\"]\n",
    "        )\n",
    "    else:\n",
    "        msk |= population.country.isin(former_to_current[former]) & (\n",
    "            population.year <= date_window[former][\"max\"]\n",
    "        )\n",
    "population = population[\n",
    "    (population.year >= fe_bulk.year.min()) & (population.year <= fe_bulk.year.max())\n",
    "].astype({\"year\": int})\n",
    "population = population.loc[~msk]\n",
    "population = population.set_index([\"country\", \"year\"], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5965830-847d-41cd-a17c-558b89702c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "population = pd.concat([population, population_gap])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25152bc8-df03-4938-8e80-10b4720105dd",
   "metadata": {},
   "source": [
    "### 9.1 Normalize metrics\n",
    "In this section, we undo the _per_capita_ part of some metrics. We do this so we can aggregate countries into regions and later normalize by the total population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8970b-d2ac-4c8b-b811-33bc7e7fcd8a",
   "metadata": {},
   "source": [
    "#### Add population column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7949cc-12df-4ae3-bd8a-82c0c2f151f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_pop = set(population.index.levels[0])\n",
    "countries = set(fe_bulk.country)\n",
    "print(\n",
    "    f\"Missing {len(countries_missing := countries.difference(countries_pop))} countries: {countries_missing}\"\n",
    ")\n",
    "if len(countries_missing) > 17:\n",
    "    raise ValueError(\"More countries missing than expected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52801a-e665-424f-a21d-c03b65b5bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_first = fe_bulk.shape[0]\n",
    "fe_bulk = fe_bulk.merge(\n",
    "    population, left_on=[\"country\", \"year\"], right_on=[\"country\", \"year\"]\n",
    ")\n",
    "print(f\"Decrease of {round(100*(1-fe_bulk.shape[0]/shape_first))}% rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1b96f-a0b4-478a-9a70-7037b2f386c0",
   "metadata": {},
   "source": [
    "#### Weight columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b391137-2642-45f7-bedc-34dbc8a33710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns will be weighted\n",
    "keyword = \"_per_capita\"\n",
    "columns_per_capita = {\n",
    "    col: col.replace(keyword, \"\") for col in fe_bulk.columns if keyword in col\n",
    "}\n",
    "# Normalize and rename columns\n",
    "fe_bulk[list(columns_per_capita)] = fe_bulk[list(columns_per_capita)].multiply(\n",
    "    fe_bulk[\"population\"], axis=0\n",
    ")\n",
    "fe_bulk = fe_bulk.rename(columns=columns_per_capita).drop(columns=[\"population\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69f72c-a51b-4162-b2de-d255159a9ccb",
   "metadata": {},
   "source": [
    "### 9.2 Add regions\n",
    "Here we obtain the metrics for each region (continents, income groups and World). We avoid computing the aggregates for metrics relative to land use and animal use, as for these we would need the number of land and animals used per country. We can estimate `yield__tonnes_per_ha`, with other available metrics but will leave `yield__kg_per_animal` as NaN for all regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b5b4b-1537-4782-9350-9f2e95069e00",
   "metadata": {},
   "source": [
    "#### Create mappings Country ---> Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02921d82-0454-4c1a-837a-0c615b7b94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load region map\n",
    "with open(PATH_REGIONS, \"r\") as f:\n",
    "    regions = json.load(f)\n",
    "regions_all = [\"World\"] + list(regions)\n",
    "\n",
    "income = [\n",
    "    \"High-income countries\",\n",
    "    \"Low-income countries\",\n",
    "    \"Lower-middle-income countries\",\n",
    "    \"Upper-middle-income countries\",\n",
    "]\n",
    "continents = [\n",
    "    \"Antarctica\",\n",
    "    \"Africa\",\n",
    "    \"Asia\",\n",
    "    \"Europe\",\n",
    "    \"South America\",\n",
    "    \"North America\",\n",
    "    \"Oceania\",\n",
    "]\n",
    "country2continent = {vv: k for k, v in regions.items() for vv in v if k in continents}\n",
    "country2income = {vv: k for k, v in regions.items() for vv in v if k in income}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ce6ef-8a61-418d-a047-bad4ca0e10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure former states presence\n",
    "country2continent[\"Sudan (former)\"] = \"Africa\"\n",
    "country2income = {\n",
    "    **country2income,\n",
    "    \"Czechoslovakia\": \"High-income countries\",\n",
    "    \"Eritrea and Ethiopia\": \"Low-income countries\",\n",
    "    \"Serbia and Montenegro\": \"Upper-middle-income countries\",\n",
    "    \"Yugoslavia\": \"Upper-middle-income countries\",\n",
    "    \"USSR\": \"Upper-middle-income countries\",\n",
    "    \"Sudan (former)\": \"Low-income countries\",\n",
    "}\n",
    "for state in former_states:\n",
    "    assert state in country2continent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246dc5b3-7fc8-4a8b-8e62-e805ce43ad9f",
   "metadata": {},
   "source": [
    "#### Remove default regions (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4a30b-fba6-434b-84ed-bf556ae74146",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_bulk = fe_bulk.loc[~fe_bulk.country.isin(regions_all)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e13768-a605-4216-a709-a70dbc4e1240",
   "metadata": {},
   "source": [
    "#### Function and variables to get metrics for regions\n",
    "Definition of functions recurrently needed and some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980cecc4-5c3b-4c09-90b9-5e2d5873eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_regions(df, mapping, column_location, columns_index, columns_aggregate=None):\n",
    "    # TODO: flag whenever all (production__tonnes, area_harvested__ha) are available\n",
    "    # Continents\n",
    "    df_regions = df.assign(**{column_location: df[column_location].replace(mapping)})\n",
    "    if columns_aggregate is not None:\n",
    "        df_regions = df_regions.groupby(columns_index, as_index=False)[\n",
    "            columns_aggregate\n",
    "        ].sum(min_count=1)\n",
    "    else:\n",
    "        df_regions = df_regions.groupby(columns_index, as_index=False).sum(min_count=1)\n",
    "    # Only keep new regions\n",
    "    msk = df_regions[column_location].isin(set(mapping.values()))\n",
    "    df_regions = df_regions.loc[msk]\n",
    "    print(f\"{round(100*df_regions.shape[0]/df.shape[0], 2)}% increase in rows\")\n",
    "    return df_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eca576-ca7d-44cd-8def-9f5232a93fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_index = [\"product\", \"country\", \"year\"]\n",
    "columns_exclude = columns_index + [\"yield__tonnes_per_ha\", \"yield__kg_per_animal\"]\n",
    "columns_aggregate = [col for col in fe_bulk.columns if col not in columns_exclude]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df5e37-d7a7-453c-8bb2-1fe2ceafe4be",
   "metadata": {},
   "source": [
    "#### Estimate region data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9543c5-f007-4c23-adda-ee9caa9ced11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# World\n",
    "fe_bulk_world = (\n",
    "    fe_bulk.groupby([\"product\", \"year\"], as_index=False)[columns_aggregate]\n",
    "    .sum(min_count=1)\n",
    "    .assign(country=\"World\")\n",
    ")\n",
    "print(f\"{round(100*fe_bulk_world.shape[0]/fe_bulk.shape[0], 2)}% increase in rows\")\n",
    "# Continents\n",
    "fe_bulk_continent = get_df_regions(\n",
    "    fe_bulk, country2continent, \"country\", columns_index, columns_aggregate\n",
    ")\n",
    "# Income groups\n",
    "fe_bulk_income = get_df_regions(\n",
    "    fe_bulk, country2income, \"country\", columns_index, columns_aggregate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc8d2f-61a5-4daa-b11b-7e8fef8a6400",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5d1b2-7c08-4cf3-ab72-7215e9dc14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "fe_bulk = pd.concat([fe_bulk, fe_bulk_world, fe_bulk_continent, fe_bulk_income])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7dfc2c-26a1-4f3d-9f2f-5fac605f205d",
   "metadata": {},
   "source": [
    "#### Add missing metrics for regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ccaa0-36c4-4c25-9991-a51fb5a04827",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = (\n",
    "    (fe_bulk.country.isin(regions_all))\n",
    "    & (fe_bulk[\"area_harvested__ha\"] != 0)\n",
    "    & (~fe_bulk[\"area_harvested__ha\"].isna())\n",
    ")\n",
    "fe_bulk.loc[msk, \"yield__tonnes_per_ha\"] = (\n",
    "    fe_bulk.loc[msk, \"production__tonnes\"] / fe_bulk.loc[msk, \"area_harvested__ha\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea01c77-bf46-4853-88d3-c0e69b93a600",
   "metadata": {},
   "source": [
    "### 9.3 Population\n",
    "Next, we will add a column with the population of each country (or region). Note that some regions are not present in the population dataset, hence we first need to add these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b04f6-2bdf-42cb-95c7-f30b21db6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population dataset\n",
    "population = population.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936b830-050d-44c1-8a47-84ee331d6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove regions\n",
    "population = population[~population.country.isin(set(country2continent.values()))]\n",
    "# Remove income groups\n",
    "population = population[~population.country.isin(set(country2income.values()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfacedc6-ca87-4ffb-8d27-9c49fe6aa929",
   "metadata": {},
   "source": [
    "#### Obtain continent and income group populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba04bbd-8650-4dbe-9797-4512f599ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_continent = get_df_regions(\n",
    "    population, country2continent, \"country\", [\"country\", \"year\"]\n",
    ")\n",
    "population_income = get_df_regions(\n",
    "    population, country2income, \"country\", [\"country\", \"year\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb6107-13df-43ce-b7d2-be58a8a56fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "population = pd.concat([population, population_continent, population_income])\n",
    "population = population.set_index([\"country\", \"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e627d-9d24-450b-887c-b032c7c8ab58",
   "metadata": {},
   "source": [
    "#### Add `population` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38505ff2-c5ff-4c29-8a19-57231652ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_bulk = fe_bulk.merge(population, left_on=[\"country\", \"year\"], right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304228bf-e4d3-40e7-8b63-facc0f329c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_bulk = fe_bulk.set_index(\n",
    "    [\"product\", \"country\", \"year\"], verify_integrity=True\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15470f-e366-458c-92e1-a0c94896ab72",
   "metadata": {},
   "source": [
    "### 9.4 Value checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc91b5-f61e-4fc7-b16b-21b3703f10f2",
   "metadata": {},
   "source": [
    "#### Remove values for _food_available_for_consumption__kcal_per_day\n",
    "We remove values for metric `food_available_for_consumption__kcal_per_day` whenever they seem wrong. Our criteria is to find out if for a given `(item,country)` this metric only has few values. We define _few_ as below a pre-defined threshold `th`.\n",
    "\n",
    "Note, here removing means assigning `NaN` to this metric for the rows considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425a7d7-c007-4513-a58f-5cde33af7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of the distribution of different metric values\n",
    "res = fe_bulk.groupby(\n",
    "    [fe_bulk.index.get_level_values(0), fe_bulk.index.get_level_values(1)]\n",
    ").food_available_for_consumption__kcal_per_day.nunique()\n",
    "res[res != 0].value_counts(normalize=True).cumsum().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427afb2-b765-4019-9056-af14ab3ed983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get valid (item,country)\n",
    "threshold = 5\n",
    "idx_keep = res[res < threshold].index\n",
    "# Assign NaNs\n",
    "index_2 = pd.Index([i[:2] for i in fe_bulk.index])\n",
    "msk = index_2.isin(idx_keep)\n",
    "fe_bulk.loc[msk, \"food_available_for_consumption__kcal_per_day\"] = pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c6d80-3919-4afb-bd25-88807c9da820",
   "metadata": {},
   "source": [
    "#### Remove outliers\n",
    "Remove outliers (i.e. subsitute the values with `NaN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8042ec6-6c13-44b4-89e3-00d9e1b5c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define for each column (metric) which indices should be 'removed'\n",
    "with open(PATH_OUTLIERS, \"r\") as f:\n",
    "    outliers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4f91a-308d-4d1c-b2f5-041cfe7c4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for datapoints in outliers:\n",
    "    fe_bulk.loc[datapoints[\"index\"], datapoints[\"column\"]] = pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10335b3-24ef-4f0b-8822-e91d2ef5e67f",
   "metadata": {},
   "source": [
    "### 9.5 Correct region entities\n",
    "For some `product`, `metric` and `year` no value can be estimated for certain regions. This is because a big chunk of the region's population (i.e. countries) are missing. In this section we filter these entries out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047dde7-60c4-409f-bccc-6d486562d82f",
   "metadata": {},
   "source": [
    "For this processing step, we melt the dataframe and divide it into two parts: \n",
    "- Country data \n",
    "- Region data (continents, income groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea3d6e-a66c-422f-a10a-3adba1d52c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_bulk_orig = fe_bulk.copy()\n",
    "fe_bulk_melted = fe_bulk.reset_index().melt(\n",
    "    id_vars=[\"product\", \"country\", \"year\", \"population\"], var_name=\"metric\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717376e5-9749-4694-a9d9-a123ec213b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nan values\n",
    "fe_bulk_melted = fe_bulk_melted.dropna(subset=\"value\")\n",
    "# Exclude regions\n",
    "regions_ = continents + income + [\"World\"]\n",
    "msk = fe_bulk_melted.country.isin(regions_)\n",
    "fe_bulk_melted_countries = fe_bulk_melted[~msk]\n",
    "fe_bulk_melted_regions = fe_bulk_melted[msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc6b22b-76c4-48b6-940a-976e1f3bbdc2",
   "metadata": {},
   "source": [
    "Next, we build a dataframe `x` which contains the _population difference_ for each region given a product, metric and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08869c-95ff-4d7e-bda6-ded79a93aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(x, ncountries=True):\n",
    "    # add number of countries and population in present countries\n",
    "    population_ = (\n",
    "        x.groupby([\"product\", \"metric\", \"region\", \"year\"]).population.sum().tolist()\n",
    "    )\n",
    "    x = x.groupby(\n",
    "        [\"product\", \"metric\", \"region\", \"year\"], as_index=False\n",
    "    ).country.nunique()\n",
    "    x = x.assign(\n",
    "        population=population_,\n",
    "    )\n",
    "\n",
    "    # add real population\n",
    "    population_ = population.reset_index().astype({\"year\": float})\n",
    "    x = x.merge(\n",
    "        population_, left_on=[\"region\", \"year\"], right_on=[\"country\", \"year\"]\n",
    "    ).rename(columns={\"population_y\": \"population_gt\", \"population_x\": \"population\"})\n",
    "    if ncountries:\n",
    "        # add real number of countries\n",
    "        region_size = []\n",
    "        for r, members in regions.items():\n",
    "            region_size.append({\"region\": r, \"ncountries_gt\": len(members)})\n",
    "        r = pd.DataFrame(region_size)\n",
    "        x = x.merge(r, left_on=\"region\", right_on=\"region\")\n",
    "    # build df\n",
    "    x = pd.DataFrame(x)\n",
    "    # diff population\n",
    "    x = x.assign(\n",
    "        population_diff=x.population_gt - x.population,\n",
    "        population_diff_perc=(x.population_gt - x.population) / x.population_gt,\n",
    "    )\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711580c4-d34c-426f-b49d-56b45defd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continents\n",
    "x_cont = build_df(\n",
    "    fe_bulk_melted_countries.assign(\n",
    "        region=fe_bulk_melted_countries.country.map(country2continent)\n",
    "    )\n",
    ")\n",
    "# income groups\n",
    "x_inco = build_df(\n",
    "    fe_bulk_melted_countries.assign(\n",
    "        region=fe_bulk_melted_countries.country.map(country2income)\n",
    "    )\n",
    ")\n",
    "# world\n",
    "x_world = build_df(fe_bulk_melted_countries.assign(region=\"World\"), ncountries=False)\n",
    "# merge\n",
    "x = pd.concat([x_cont, x_inco, x_world], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb188b-9b10-4447-90ee-f663876c06b4",
   "metadata": {},
   "source": [
    "We now merge `x` with `fe_bulk_melted_regions` and filter out all entries that have a `population difference` greater than `t1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7b18a-17a3-446e-84aa-85b1d0d75a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "cols_merge = [\"product\", \"region\", \"year\", \"metric\"]\n",
    "fe_bulk_melted_regions = fe_bulk_melted_regions.merge(\n",
    "    x[cols_merge + [\"population\", \"population_diff_perc\"]],\n",
    "    left_on=[\"product\", \"country\", \"year\", \"metric\"],\n",
    "    right_on=[\"product\", \"region\", \"year\", \"metric\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "fe_bulk_melted_regions = fe_bulk_melted_regions.rename(\n",
    "    columns={\"population_x\": \"population\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ff5ec-1c3a-427c-96e5-8c7fa39571f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checks after merge\n",
    "msk = fe_bulk_melted_regions.isna().any(axis=1)\n",
    "values_to_remove = fe_bulk_melted_regions.loc[msk, \"value\"].unique()\n",
    "if not all(values_to_remove == [0.011428571428571429, 0.0]) or msk.sum() > 60:\n",
    "    raise ValueError(f\"Re-check merge: {msk.sum()}, {values_to_remove}\")\n",
    "# Filter NaNs (controlled)\n",
    "fe_bulk_melted_regions = fe_bulk_melted_regions[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368db8c1-2f71-4cd1-917d-8b69a1e63a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all samples with > T1\n",
    "## Threshold\n",
    "t1 = 0.24  # Selected such that no datapoint for product='Total' is lost\n",
    "t1_backup = fe_bulk_melted_regions[\n",
    "    (fe_bulk_melted_regions[\"product\"] == \"Total\")\n",
    "].population_diff_perc.max()\n",
    "assert t1 > t1_backup\n",
    "## Only apply to these metrics\n",
    "metrics = [\n",
    "    \"food_available_for_consumption__fat_g_per_day\",\n",
    "    \"food_available_for_consumption__kcal_per_day\",\n",
    "    \"food_available_for_consumption__kg_per_year\",\n",
    "    \"food_available_for_consumption__protein_g_per_day\",\n",
    "    \"other_uses__tonnes\",\n",
    "    \"waste_in_supply_chain__tonnes\",\n",
    "    \"feed__tonnes\",\n",
    "]\n",
    "\n",
    "fe_bulk_melted_regions = fe_bulk_melted_regions[\n",
    "    ~(\n",
    "        (fe_bulk_melted_regions.population_diff_perc >= t1)\n",
    "        & (fe_bulk_melted_regions.metric.isin(metrics))\n",
    "    )\n",
    "    | ((fe_bulk_melted_regions[\"product\"] == \"Total\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420ba0b-0f07-4fa4-8196-f92f724eda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix population for > 0\n",
    "fe_bulk_melted_regions = fe_bulk_melted_regions.assign(\n",
    "    population_per_capita=fe_bulk_melted_regions.population\n",
    ")\n",
    "msk = (fe_bulk_melted_regions.population_per_capita > 0) & (\n",
    "    fe_bulk_melted_regions.metric.isin(metrics)\n",
    ")\n",
    "fe_bulk_melted_regions.loc[msk, \"population_per_capita\"] = fe_bulk_melted_regions.loc[\n",
    "    msk, \"population_y\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae02b8-0c1a-4d82-af03-62148aa36e9c",
   "metadata": {},
   "source": [
    "Next, we estimate per capita values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c60cc1-0ff2-4a75-9b5d-344337e37df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate per_capita\n",
    "fe_bulk_melted_regions = pd.DataFrame(fe_bulk_melted_regions)\n",
    "fe_bulk_melted_regions = fe_bulk_melted_regions.assign(\n",
    "    metric_capita=fe_bulk_melted_regions.metric + \"__per_capita\",\n",
    "    value_capita=fe_bulk_melted_regions.value\n",
    "    / fe_bulk_melted_regions.population_per_capita,\n",
    ")\n",
    "fe_bulk_melted_countries = pd.DataFrame(fe_bulk_melted_countries)\n",
    "fe_bulk_melted_countries = fe_bulk_melted_countries.assign(\n",
    "    metric_capita=fe_bulk_melted_countries.metric + \"__per_capita\",\n",
    "    value_capita=fe_bulk_melted_countries.value / fe_bulk_melted_countries.population,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183f73b-6e8f-4d82-abc5-faae45a47b2c",
   "metadata": {},
   "source": [
    "Time to pivot back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0bb18b-fe9b-4aca-ac57-03fb90b9f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"product\",\n",
    "    \"country\",\n",
    "    \"year\",\n",
    "    \"metric\",\n",
    "    \"population\",\n",
    "    \"value\",\n",
    "    \"metric_capita\",\n",
    "    \"value_capita\",\n",
    "]\n",
    "r = pd.concat(\n",
    "    [fe_bulk_melted_countries[cols], fe_bulk_melted_regions[cols]], ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0f579-a5f0-45d9-845d-4cd56397ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot\n",
    "fe_bulk_absolute = (\n",
    "    r.pivot(\n",
    "        index=[\"product\", \"country\", \"year\", \"population\"],\n",
    "        columns=\"metric\",\n",
    "        values=\"value\",\n",
    "    )\n",
    "    .reset_index()\n",
    "    .set_index([\"product\", \"country\", \"year\"])\n",
    ")\n",
    "fe_bulk_capita = (\n",
    "    r.pivot(\n",
    "        index=[\"product\", \"country\", \"year\", \"population\"],\n",
    "        columns=\"metric_capita\",\n",
    "        values=\"value_capita\",\n",
    "    )\n",
    "    .reset_index()\n",
    "    .set_index([\"product\", \"country\", \"year\"])\n",
    "    .drop(columns=[\"population\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b879f-3a08-4254-89ea-f234c9a267ac",
   "metadata": {},
   "source": [
    "Build `fe_bulk` back again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a4986-945a-44e8-96e7-ba49f4d7df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_bulk = pd.merge(\n",
    "    fe_bulk_absolute, fe_bulk_capita, left_index=True, right_index=True, how=\"outer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabffca-e73f-4d51-933a-c2f16415846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "# fe_bulk.loc[\"Maize\", \"Asia\"][\"food_available_for_consumption__kcal_per_day__per_capita\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c32576-5034-4999-9704-33977288dd99",
   "metadata": {},
   "source": [
    "### 9.6 Remove former countries\n",
    "We want the values reported for former states to account for regions (continents, income groups), but not that they appear on themselves on the explorer. Therefore, we eliminate these from the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b752d48-3e7d-4b63-8903-47c9d5f7d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_bulk = fe_bulk.reset_index()\n",
    "fe_bulk = fe_bulk[~fe_bulk.country.isin(former_states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e4733-e137-476a-920b-bf0bfc97d0e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eae09b-a6f3-4b4e-94e8-bbbb35cd4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_bulk = fe_bulk.set_index(\n",
    "    [\"product\", \"country\", \"year\"], verify_integrity=True\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f99ec-e861-4296-b1ea-0ded650c34a2",
   "metadata": {},
   "source": [
    "### 9.7 Remove unnused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a722e2-7f62-49f9-9325-c2205af75e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnused columns (https://github.com/owid/etl/pull/134#issuecomment-1076883200)\n",
    "columns_remove = [\n",
    "    \"food_available_for_consumption__fat_g_per_day\",\n",
    "    \"food_available_for_consumption__kcal_per_day\",\n",
    "    \"food_available_for_consumption__kg_per_year\",\n",
    "    \"food_available_for_consumption__protein_g_per_day\",\n",
    "    \"yield__kg_per_animal__per_capita\",\n",
    "    \"yield__tonnes_per_ha__per_capita\",\n",
    "]\n",
    "\n",
    "fe_bulk = fe_bulk.drop(columns=columns_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b10ac-5ba4-497d-8dc0-bd4b5b487d85",
   "metadata": {},
   "source": [
    "### 9.8 Remove all zero series\n",
    "Here we detect all `(country, product, metric)` which timeseries is all zeroes and set it to `NaN`. This way, this metric will be ignored in Grapher for the given country and product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc60a5c-3e0c-471a-928b-e7aaf54d48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivot\n",
    "x = fe_bulk.melt(var_name=\"metric\", ignore_index=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddc833-e60d-4788-ba6b-ba112ec4ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find (product, country, metric) with all zeros (or NaNs)\n",
    "res = x.groupby([\"product\", \"country\", \"metric\"]).agg(\n",
    "    value_sum=(\"value\", \"sum\"), value_nunique=(\"value\", \"nunique\")\n",
    ")\n",
    "msk = (\n",
    "    (res[\"value_nunique\"] == 1)\n",
    "    & (res[\"value_sum\"] == 0)\n",
    "    & (res.index.get_level_values(2) != \"population\")\n",
    ")\n",
    "idx = msk[msk == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351c361-4d87-4ee2-8ce7-a223fafb7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with NaNs\n",
    "xx = x.set_index([\"product\", \"country\", \"metric\"])\n",
    "xx.loc[idx, \"value\"] = np.nan\n",
    "xx = xx.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af8931-42cb-4f62-8d4e-d0c3369b855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot back\n",
    "fe_bulk = xx.pivot([\"product\", \"country\", \"year\"], \"metric\", \"value\").astype(\n",
    "    fe_bulk.dtypes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b30b62-ddb4-4ed5-bc00-5ba6b71f6ba4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10. Export\n",
    "Time to export the shining brand new dataset!\n",
    "\n",
    "We export it in two flavours: bulk and file-per-product formats. The former is the standard format, while the later is intended to power OWID tools such as explorers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161da3a4-3733-4ee8-878c-c66813ce9cba",
   "metadata": {},
   "source": [
    "### Define metadata\n",
    "Prior to export, we need to create the metadata content for this dataset. It basically propagates the metadata from its building pieces (QCL so far).\n",
    "\n",
    "For this dataset, we use namespace `explorers`, which is intended for datasets aimed at powering explorers (this may change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11a2df-9018-4db6-a741-c8e23f7fcc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owid.catalog.meta import DatasetMeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d7187-b99d-4c17-a726-d9e44a59806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "fe_garden = catalog.Dataset.create_empty(dest_dir)\n",
    "fe_garden.metadata = DatasetMeta(\n",
    "    namespace=\"explorers\",\n",
    "    short_name=\"food_explorer\",\n",
    "    title=\"Food Explorer: Livestock & Crops, Food Balances - FAO (2017, 2021)\",\n",
    "    description=(\n",
    "        \"This dataset has been created by Our World in Data, merging existing FAOstat datsets. In particular, we have used 'Crops and livestock products' (QCL) and 'Food Balances' (FBSH and FBS) datasets. Each row contains all the \"\n",
    "        \"metrics for a specific combination of (country, product, year). The metrics may come from different datasets.\"\n",
    "    ),\n",
    "    sources=qcl_garden.metadata.sources + fbsc_garden.metadata.sources,\n",
    "    licenses=qcl_garden.metadata.licenses + fbsc_garden.metadata.licenses,\n",
    ")\n",
    "fe_garden.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d450d2-5169-4c10-8d12-fa825608a6a9",
   "metadata": {},
   "source": [
    "### In bulk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22417050-7cd1-40cf-8762-9037241afd11",
   "metadata": {},
   "source": [
    "Preserve the bulk file for QA or manual analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a52c5d-62d0-4962-b3ec-04d668baa984",
   "metadata": {},
   "source": [
    "#### Create metadata for fields\n",
    "Here we create the content for `field` metadata field, which contains metric-specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae944be0-59ce-4023-b29a-d8c5b26d62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table from dataset containing Element information\n",
    "qcl_elem = qcl_garden[\"meta_qcl_element\"]\n",
    "fbsc_elem = fbsc_garden[\"meta_fbs_element\"]\n",
    "qcl_elem[\"name_std\"] = qcl_elem.index.map(map_elem)\n",
    "fbsc_elem[\"name_std\"] = fbsc_elem.index.map(map_elem)\n",
    "element_metadata = pd.concat(\n",
    "    [qcl_elem.dropna().assign(dataset=\"QCL\"), fbsc_elem.dropna().assign(dataset=\"FBS\")]\n",
    ")\n",
    "# Final patch\n",
    "patch = {\n",
    "    \"food_available_for_consumption__fat_g_per_day_per_capita\": \"food_available_for_consumption__fat_g_per_day\",\n",
    "    \"food_available_for_consumption__protein_g_per_day_per_capita\": \"food_available_for_consumption__protein_g_per_day\",\n",
    "    \"food_available_for_consumption__kcal_per_day_per_capita\": \"food_available_for_consumption__kcal_per_day\",\n",
    "    \"food_available_for_consumption__kg_per_capita_per_year\": \"food_available_for_consumption__kg_per_year\",\n",
    "}\n",
    "element_metadata[\"name_std\"] = element_metadata[\"name_std\"].replace(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd935a-95b3-4433-b7ae-401e2f96f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'easy' fields\n",
    "def _get_source_ids(dataset_code):\n",
    "    res = [\n",
    "        i\n",
    "        for i, source in enumerate(fe_garden.metadata.sources)\n",
    "        if f\"{dataset_code}\" in source.owid_data_url\n",
    "    ]\n",
    "    return res\n",
    "\n",
    "\n",
    "def _build_description_extra(fe_bulk, col):\n",
    "    num_products = len(set(fe_bulk[col].dropna().index.get_level_values(0)))\n",
    "    num_countries = len(set(fe_bulk[col].dropna().index.get_level_values(1)))\n",
    "    description = f\"This metric is present in {num_products} products and {num_countries} countries.\"\n",
    "    return description\n",
    "\n",
    "\n",
    "def _get_sources_and_licenses(dataset_code):\n",
    "    source_ids = _get_source_ids(dataset_code)\n",
    "    sources = [fe_garden.metadata.sources[i] for i in source_ids]\n",
    "    licenses = [fe_garden.metadata.licenses[i] for i in source_ids]\n",
    "    return {\"sources\": sources, \"licenses\": licenses}\n",
    "\n",
    "\n",
    "fields = {}\n",
    "columns = list(fe_bulk.columns) + fe_bulk.index.names\n",
    "for col in columns:\n",
    "    msk = element_metadata.name_std == col\n",
    "    if msk.sum() == 0:\n",
    "        if \"__per_capita\" in col:\n",
    "            msk = element_metadata.name_std == col.replace(\"__per_capita\", \"\")\n",
    "        if msk.sum() == 0:\n",
    "            msk = element_metadata.name_std == f\"{col}_per_capita\"\n",
    "\n",
    "    if msk.sum() == 1:\n",
    "        dataset_code = element_metadata.loc[msk, \"dataset\"].item()\n",
    "        description = element_metadata.loc[msk, \"description\"].item()\n",
    "        fields[col] = catalog.VariableMeta(\n",
    "            title=\"\",\n",
    "            description=description,\n",
    "            **_get_sources_and_licenses(dataset_code),\n",
    "            display={\"description_extra\": _build_description_extra(fe_bulk, col)},\n",
    "        )\n",
    "    elif msk.sum() > 1:\n",
    "        dataset_codes = element_metadata.loc[msk, \"dataset\"]\n",
    "        if dataset_codes.nunique() != 1:\n",
    "            raise ValueError(\n",
    "                f\"Merged metrics should all be from the same dataset! Check {col}\"\n",
    "            )\n",
    "        dataset_code = dataset_codes.unique()[0]\n",
    "        fields[col] = catalog.VariableMeta(\n",
    "            title=\"\",\n",
    "            description=\"\",\n",
    "            **_get_sources_and_licenses(dataset_code),\n",
    "            display={\"description_extra\": _build_description_extra(fe_bulk, col)},\n",
    "        )\n",
    "    else:\n",
    "        fields[col] = catalog.VariableMeta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06fdf2-4ae4-4eff-aae2-76d000fcf1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing fields\n",
    "cols_missing = [f for f, v in fields.items() if v.description == \"\"]\n",
    "cols_missing_check = {\n",
    "    \"exports__tonnes\",\n",
    "    \"imports__tonnes\",\n",
    "    \"producing_or_slaughtered_animals__animals\",\n",
    "    \"yield__kg_per_animal\",\n",
    "    \"exports__tonnes__per_capita\",\n",
    "    \"food_available_for_consumption__fat_g_per_day__per_capita\",\n",
    "    \"food_available_for_consumption__kcal_per_day__per_capita\",\n",
    "    \"food_available_for_consumption__kg_per_year__per_capita\",\n",
    "    \"food_available_for_consumption__protein_g_per_day__per_capita\",\n",
    "    \"imports__tonnes__per_capita\",\n",
    "    \"producing_or_slaughtered_animals__animals__per_capita\",\n",
    "}\n",
    "assert set(cols_missing) == cols_missing_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b53f3-2589-4094-8198-982664ff7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields['exports__tonnes']['description'] =\n",
    "# fields['imports__tonnes']['description'] =\n",
    "# fields['producing_or_slaughtered_animals__animals']['description'] =\n",
    "# fields['yield__kg_per_animal']['description'] = \"Yield is measured as the quantity produced per unit area of land used to grow it.\"\n",
    "# fields['food_available_for_consumption__fat_g_per_day']['description'] =\n",
    "# fields['food_available_for_consumption__kcal_per_day']['description'] =\n",
    "# fields['food_available_for_consumption__kg_per_year']['description'] =\n",
    "# fields['food_available_for_consumption__protein_g_per_day']['description'] ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef940581-a4c0-4b37-8260-c398109916bf",
   "metadata": {},
   "source": [
    "#### Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bedabf-2d57-4789-9466-a66ac2f28aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = catalog.Table(fe_bulk)\n",
    "t.metadata.short_name = \"bulk\"\n",
    "t._fields = fields\n",
    "fe_garden.add(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34ed58-fec7-4179-adb7-5459e230542d",
   "metadata": {},
   "source": [
    "### One file per product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec2879-2494-4ed8-969b-15d3fbd1c52b",
   "metadata": {},
   "source": [
    "To work in an explorer, we need to add the table in CSV format. To make it more scalable for use, we want\n",
    "to split that dataset into many small files, one per product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2711acb-c700-4ce9-89f5-6fbbf7c9a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_short_name(raw):\n",
    "    return (\n",
    "        raw.lower()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\".\", \"\")\n",
    "    )\n",
    "\n",
    "\n",
    "# the index contains values like \"Asses\" which have already been filtered out from the data,\n",
    "# let's remove them\n",
    "fe_bulk.index = fe_bulk.index.remove_unused_levels()\n",
    "\n",
    "for product in sorted(fe_bulk.index.levels[0]):\n",
    "    short_name = to_short_name(product)\n",
    "    print(f\"{product} --> {short_name}.csv\")\n",
    "\n",
    "    t = catalog.Table(fe_bulk.loc[[product]])\n",
    "    t.metadata.short_name = short_name\n",
    "    fe_garden.add(t, format=\"csv\")  # <-- note we choose CSV format here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e70d0c-bdd4-4ea5-acb2-6a4ae475d872",
   "metadata": {},
   "source": [
    "Let's check that the biggest files are still an ok size for an explorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042dcdd-2cc2-460b-9523-e3167bcaef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -hs {dest_dir}/*.csv | sort -hr | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b391ff9-3fc7-44ce-a292-0b0c83e792d5",
   "metadata": {},
   "source": [
    "The biggest is 3.1MB (csv), we should be ok  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1135eb4-d0a2-4dfb-9e5a-881ff643c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparison with previous (live) export\n",
    "# product = 'vegetables'\n",
    "# df_new = pd.read_csv(f'/tmp/food_explorer/{product}.csv')\n",
    "# df_old = pd.read_csv(f'https://owid-catalog.nyc3.digitaloceanspaces.com/garden/explorers/2021/food_explorer/{product}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24302bf7-a762-4b5b-b9f6-787f6fac58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot metric\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = [10, 7]\n",
    "# plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "# metric = \"food_available_for_consumption__kcal_per_day\"\n",
    "# # country = \"Europe\"\n",
    "# country = \"High-income countries\"\n",
    "# product = \"Total\"\n",
    "# (\n",
    "#     fe_bulk.loc[(product, country), metric]\n",
    "#     / fe_bulk.loc[(product, country), \"population\"]\n",
    "# ).plot(x=\"year\", title=f\"Food Supply in {country} ({product})\", ylim=[0,3500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad21b95-61a1-430c-829c-8ced49cf10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for former, current in former_to_current.items():\n",
    "#     print(former)\n",
    "#     for c in current:\n",
    "#         print(c, country2income[c])\n",
    "#     print('---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
