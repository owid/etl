"""
Load a meadow dataset and create a garden dataset.

When running this step in an update, be sure to check all the outputs and logs to ensure the data is correct.

NOTE: To extract the log of the process (to review sanity checks, for example), run the following command in the terminal:
    nohup uv run etl run world_bank_pip > output.log 2>&1 &

"""

from typing import List, Tuple

import numpy as np
import owid.catalog.processing as pr
import pandas as pd
from owid.catalog import Table
from structlog import get_logger
from tabulate import tabulate

from etl.data_helpers import geo
from etl.helpers import PathFinder

# Get paths and naming conventions for current step.
paths = PathFinder(__file__)

# Initialize logger.
log = get_logger()

# Define absolute poverty lines used depending on PPP version
# NOTE: Modify if poverty lines are updated from source
# TODO: Modify the lines in 2021 prices
POVLINES_DICT = {
    2011: ["100", "190", "320", "550", "1000", "2000", "3000", "4000"],
    2017: ["100", "215", "365", "685", "1000", "2000", "3000", "4000"],
}

# Define PPP versions from POVLINES_DICT
PPP_VERSIONS = list(POVLINES_DICT.keys())

PPP_YEAR_OLD = PPP_VERSIONS[0]
PPP_YEAR_CURRENT = PPP_VERSIONS[1]

# Define international poverty lines as the second value in each list in POVLINES_DICT
INTERNATIONAL_POVERTY_LINES = {ppp_year: poverty_lines[1] for ppp_year, poverty_lines in POVLINES_DICT.items()}

# Set precision of sanity checks for percentages
PRECISION_PERCENTAGE = 0.1

# Define regions in the dataset
REGIONS_LIST = [
    "East Asia and Pacific (PIP)",
    "Eastern and Southern Africa (PIP)",
    "Europe and Central Asia (PIP)",
    "Latin America and the Caribbean (PIP)",
    "Middle East and North Africa (PIP)",
    "Other high income countries (PIP)",
    "South Asia (PIP)",
    "Sub-Saharan Africa (PIP)",
    "Western and Central Africa (PIP)",
    "World",
    "World (excluding China)",
    "World (excluding India)",
]

# Define countries expected to have both income and consumption data
COUNTRIES_WITH_INCOME_AND_CONSUMPTION = [
    "Albania",
    "Armenia",
    "Belarus",
    "Belize",
    "Bulgaria",
    "China",
    "China (rural)",
    "China (urban)",
    "Croatia",
    "Estonia",
    "Haiti",
    "Hungary",
    "Kazakhstan",
    "Kyrgyzstan",
    "Latvia",
    "Lithuania",
    "Montenegro",
    "Namibia",
    "Nepal",
    "Nicaragua",
    "North Macedonia",
    "Peru",
    "Philippines",
    "Poland",
    "Romania",
    "Russia",
    "Saint Lucia",
    "Serbia",
    "Seychelles",
    "Slovakia",
    "Slovenia",
    "Turkey",
    "Ukraine",
]

# Set table format when printing
TABLEFMT = "pretty"

# Define indicators that don't depend on poverty lines
INDICATORS_NOT_DEPENDENT_ON_POVLINES_NOR_DECILES = [
    "mean",
    "median",
    "mld",
    "gini",
    "polarization",
    "cpi",
    "ppp",
    "reporting_pop",
    "reporting_gdp",
    "reporting_pce",
    "spl",
    "spr",
    "pg",
    "estimate_type",
    "pop_in_poverty",
    "bottom_50_share",
    "middle_40_share",
    "palma_ratio",
    "s80_s20_ratio",
    "p90_p10_ratio",
    "p90_p50_ratio",
    "p50_p10_ratio",
    "top1_thr",
    "top1_share",
    "top1_avg",
    "top90_99_share",
    "surveys_past_decade",
    "region_name",
]


def run() -> None:
    #
    # Load inputs.
    #
    # Load meadow dataset.
    ds_meadow = paths.load_dataset("world_bank_pip")

    # Read tables from meadow dataset.
    # Key indicators
    tb = ds_meadow.read("world_bank_pip")

    # Percentiles
    tb_percentiles = ds_meadow.read("world_bank_pip_percentiles")

    # Region definitions
    tb_region_definitions = ds_meadow.read("world_bank_pip_regions")

    #
    # Process data.
    #
    tb = create_new_indicators_and_format(tb=tb)
    tb = calculate_inequality_indicators(tb=tb)

    # Harmonize country names
    tb = geo.harmonize_countries(df=tb, countries_file=paths.country_mapping_path, warn_on_unused_countries=False)
    tb_percentiles = geo.harmonize_countries(
        df=tb_percentiles, countries_file=paths.country_mapping_path, warn_on_unused_countries=False
    )
    tb_region_definitions = harmonize_region_name(tb=tb_region_definitions)

    # Make share a percentage in tb_percentiles
    tb_percentiles["share"] *= 100

    # Add top 1 percentile to the table
    tb = add_top_1_percentile(tb=tb, tb_percentiles=tb_percentiles)

    # Show regional data from 1990 onwards
    tb = regional_data_from_1990(tb=tb, regions_list=REGIONS_LIST)
    tb_percentiles = regional_data_from_1990(tb=tb_percentiles, regions_list=REGIONS_LIST)

    # Amend the entity to reflect if data refers to urban or rural only
    tb = identify_rural_urban(tb)

    # Create stacked variables from headcount and headcount_ratio
    tb = create_stacked_variables(tb=tb)

    # NOTE: Uncomment this
    # # Sanity checks. I don't run for percentile tables because that process was done in the extraction
    # tb = sanity_checks(tb=tb)

    tb = make_distributional_indicators_long(tb)

    tb = make_relative_poverty_long(tb)

    # Separate out consumption-only, income-only. Also, create a table with both income and consumption
    tb, tb_inc_or_cons_smooth = inc_or_cons_data(tb)

    # Create survey count dataset, by counting the number of surveys available for each country in the past decade
    tb_inc_or_cons_smooth = survey_count(tb_inc_or_cons_smooth)

    # Add region definitions
    tb_inc_or_cons_smooth = add_region_definitions(
        tb=tb_inc_or_cons_smooth, tb_region_definitions=tb_region_definitions
    )

    # Concatenate the final table
    tb = pr.concat([tb, tb_inc_or_cons_smooth], ignore_index=True)

    # Drop columns not needed
    tb = drop_columns(tb)

    # Improve table format.
    tb = tb.format(
        ["country", "year", "ppp_version", "poverty_line", "welfare_type", "decile", "table", "survey_comparability"],
        short_name=paths.short_name,
    )

    #
    # Save outputs.
    #
    # Initialize a new garden dataset.
    ds_garden = paths.create_dataset(tables=[tb], default_metadata=ds_meadow.metadata)

    # Save garden dataset.
    ds_garden.save()


def create_new_indicators_and_format(tb: Table) -> Table:
    """
    Create new indicators from the existing ones and format names and shares
    """
    # rename columns
    tb = tb.rename(columns={"headcount": "headcount_ratio", "poverty_gap": "poverty_gap_index"})

    # Drop columns not needed
    tb = tb.drop(
        columns=[
            "country_code",
            "survey_acronym",
            "survey_coverage",
            "survey_year",
            "comparable_spell",
            "distribution_type",
            "estimation_type",
        ]
    )

    # Changing the decile(i) variables for decile(i)_share
    for i in range(1, 11):
        tb = tb.rename(columns={f"decile{i}": f"decile{i}_share"})

    # Calculate number in poverty
    tb["headcount"] = tb["headcount_ratio"] * tb["reporting_pop"]
    tb["headcount"] = tb["headcount"].round(0)

    # Calculate shortfall of incomes
    tb["total_shortfall"] = tb["poverty_gap_index"] * tb["poverty_line"] * tb["reporting_pop"]

    # Calculate average shortfall of incomes (averaged across population in poverty)
    tb["avg_shortfall"] = tb["total_shortfall"] / tb["headcount"]

    # Calculate income gap ratio (according to Ravallion's definition)
    tb["income_gap_ratio"] = (tb["total_shortfall"] / tb["headcount"]) / tb["poverty_line"]

    # Same for relative poverty
    for pct in [40, 50, 60]:
        tb[f"headcount_{pct}_median"] = tb[f"headcount_ratio_{pct}_median"] * tb["reporting_pop"]
        tb[f"headcount_{pct}_median"] = tb[f"headcount_{pct}_median"].round(0)
        tb[f"total_shortfall_{pct}_median"] = (
            tb[f"poverty_gap_index_{pct}_median"] * tb["median"] * pct / 100 * tb["reporting_pop"]
        )
        tb[f"avg_shortfall_{pct}_median"] = tb[f"total_shortfall_{pct}_median"] / tb[f"headcount_{pct}_median"]
        tb[f"income_gap_ratio_{pct}_median"] = (tb[f"total_shortfall_{pct}_median"] / tb[f"headcount_{pct}_median"]) / (
            tb["median"] * pct / 100
        )

    # Shares to percentages
    # executing the function over list of vars
    pct_indicators = [
        "headcount_ratio",
        "income_gap_ratio",
        "poverty_gap_index",
        "headcount_ratio_40_median",
        "headcount_ratio_50_median",
        "headcount_ratio_60_median",
        "income_gap_ratio_40_median",
        "income_gap_ratio_50_median",
        "income_gap_ratio_60_median",
        "poverty_gap_index_40_median",
        "poverty_gap_index_50_median",
        "poverty_gap_index_60_median",
    ]
    tb.loc[:, pct_indicators] = tb[pct_indicators] * 100

    # Make the poverty lines in cents for easier handling
    tb["poverty_line"] = round(tb["poverty_line"] * 100).astype(int).astype(str)

    # Round reporting_pop to int
    tb["reporting_pop"] = tb["reporting_pop"].round(0).astype(int)

    return tb


def calculate_inequality_indicators(tb: Table) -> Table:
    """
    Calculate inequality indicators: decile averages and ratios
    """

    col_decile_share = []
    col_decile_avg = []
    col_decile_thr = []

    for i in range(1, 11):
        # Because there are only 9 thresholds
        if i != 10:
            varname_thr = f"decile{i}_thr"
            col_decile_thr.append(varname_thr)

        # Define the share and average variables
        varname_share = f"decile{i}_share"
        varname_avg = f"decile{i}_avg"

        # Calculate the averages from the shares data
        tb[varname_avg] = tb[varname_share] * tb["mean"] / 0.1

        # Save the variable names to the lists
        col_decile_share.append(varname_share)
        col_decile_avg.append(varname_avg)

    # Multiplies decile columns by 100
    tb.loc[:, col_decile_share] = tb[col_decile_share] * 100

    # Create bottom 50 and middle 40% shares
    tb["bottom50_share"] = (
        tb["decile1_share"] + tb["decile2_share"] + tb["decile3_share"] + tb["decile4_share"] + tb["decile5_share"]
    )
    tb["middle40_share"] = tb["decile6_share"] + tb["decile7_share"] + tb["decile8_share"] + tb["decile9_share"]

    # Palma ratio and other average/share ratios
    tb["palma_ratio"] = tb["decile10_share"] / (
        tb["decile1_share"] + tb["decile2_share"] + tb["decile3_share"] + tb["decile4_share"]
    )
    tb["s80_s20_ratio"] = (tb["decile9_share"] + tb["decile10_share"]) / (tb["decile1_share"] + tb["decile2_share"])
    tb["p90_p10_ratio"] = tb["decile9_thr"] / tb["decile1_thr"]
    tb["p90_p50_ratio"] = tb["decile9_thr"] / tb["decile5_thr"]
    tb["p50_p10_ratio"] = tb["decile5_thr"] / tb["decile1_thr"]

    # Replace infinite values with nulls
    tb = tb.replace([np.inf, -np.inf], pd.NA)

    return tb


def harmonize_region_name(tb: Table) -> Table:
    """
    Harmonize country and region_name in tb_region_definitions, using the harmonizing tool, but removing the (PIP) suffix
    """

    tb = tb.copy()

    for country_col in ["country", "region_name"]:
        tb = geo.harmonize_countries(
            df=tb, country_col=country_col, countries_file=paths.country_mapping_path, warn_on_unused_countries=False
        )

    # Remove (PIP) from region_name
    tb["region_name"] = tb["region_name"].str.replace(r" \(PIP\)", "", regex=True)

    return tb


def add_top_1_percentile(tb: Table, tb_percentiles: Table) -> Table:
    """
    Add top 1% data (share, average, threshold) to the main indicators
    Also, calculate the share of the top 90-99%
    """

    tb = tb.copy()
    tb_percentiles = tb_percentiles.copy()

    # Create different tables for thresholds and shares/averages
    tb_percentiles_thr = tb_percentiles[tb_percentiles["percentile"] == 99].copy()
    tb_percentiles_share = tb_percentiles[tb_percentiles["percentile"] == 100].copy()

    # Select appropriate columns and rename
    tb_percentiles_thr = tb_percentiles_thr[
        ["ppp_version", "country", "year", "reporting_level", "welfare_type", "thr"]
    ]
    tb_percentiles_thr = tb_percentiles_thr.rename(columns={"thr": "top1_thr"})

    tb_percentiles_share = tb_percentiles_share[
        ["ppp_version", "country", "year", "reporting_level", "welfare_type", "share", "avg"]
    ]
    tb_percentiles_share = tb_percentiles_share.rename(columns={"share": "top1_share", "avg": "top1_avg"})

    # Merge with the main table
    tb = pr.merge(
        tb, tb_percentiles_thr, on=["ppp_version", "country", "year", "reporting_level", "welfare_type"], how="left"
    )
    tb = pr.merge(
        tb, tb_percentiles_share, on=["ppp_version", "country", "year", "reporting_level", "welfare_type"], how="left"
    )

    # Now I can calculate the share of the top 90-99%
    tb["top90_99_share"] = tb["decile10_share"] - tb["top1_share"]

    return tb


def regional_data_from_1990(tb: Table, regions_list: list) -> Table:
    """
    Select regional data only from 1990 onwards, due to the uncertainty in 1980s data
    """
    # Create a regions table
    tb_regions = tb[(tb["year"] >= 1990) & (tb["country"].isin(regions_list))].reset_index(drop=True).copy()

    # Remove regions from tb
    tb = tb[~tb["country"].isin(regions_list)].reset_index(drop=True).copy()

    # Concatenate both tables
    tb = pr.concat([tb, tb_regions], ignore_index=True)

    return tb


def identify_rural_urban(tb: Table) -> Table:
    """
    Amend the entity to reflect if data refers to urban or rural only
    """

    # Make country and reporting_level columns into strings
    tb["country"] = tb["country"].astype(str)
    tb["reporting_level"] = tb["reporting_level"].astype(str)

    # Define condition to identify urban and rural data
    condition_urban_rural = tb["reporting_level"].isin(["urban", "rural"])

    # Change the country name only for urban and rural data
    tb.loc[(condition_urban_rural), "country"] = (
        tb.loc[(condition_urban_rural), "country"] + " (" + tb.loc[(condition_urban_rural), "reporting_level"] + ")"
    )

    # Drop reporting_level column
    tb = tb.drop(columns=["reporting_level"])

    return tb


def create_stacked_variables(tb: Table) -> Tuple[Table, list, list]:
    """
    Create stacked variables from the indicators to plot them as stacked area/bar charts
    """

    tb = tb.copy()

    # Define headcount_above and headcount_ratio_above variables
    tb["headcount_above"] = tb["reporting_pop"] - tb["headcount"]
    tb["headcount_ratio_above"] = tb["headcount_above"] / tb["reporting_pop"]

    # Make headcount_ratio_above a percentage
    tb["headcount_ratio_above"] = tb["headcount_ratio_above"] * 100

    # Define stacked variables as headcount and headcount_ratio between poverty lines
    # Select only the necessary columns
    tb_pivot = tb[
        [
            "country",
            "year",
            "welfare_type",
            "ppp_version",
            "poverty_line",
            "headcount_ratio",
            "headcount",
            "reporting_pop",
        ]
    ].copy()

    # Pivot and obtain the poverty lines dictionary
    tb_pivot = pivot_and_obtain_povlines_dict(
        tb=tb_pivot,
        index=["country", "year", "welfare_type"],
        columns=["ppp_version", "poverty_line"],
    )

    for ppp_year, povlines in POVLINES_DICT.items():
        for i in range(len(povlines)):
            # if it's the first value only continue
            if i == 0:
                continue

            # If it's the last value calculate the people between this value and the previous
            # and also the people over this poverty line (and percentages)
            else:
                varname_n = ("headcount_between", ppp_year, f"{povlines[i-1]} and {povlines[i]}")
                varname_pct = ("headcount_ratio_between", ppp_year, f"{povlines[i-1]} and {povlines[i]}")
                tb_pivot[varname_n] = (
                    tb_pivot[("headcount", ppp_year, povlines[i])] - tb_pivot[("headcount", ppp_year, povlines[i - 1])]
                )
                tb_pivot[varname_pct] = tb_pivot[varname_n] / tb_pivot[("reporting_pop", ppp_year, povlines[i])]

                # Multiply by 100 to get percentage
                tb_pivot[varname_pct] = tb_pivot[varname_pct] * 100

        # Calculate stacked variables which "jump" the original order
        tb_pivot[("headcount_between", ppp_year, f"{povlines[1]} and {povlines[4]}")] = (
            tb_pivot[("headcount", ppp_year, povlines[4])] - tb_pivot[("headcount", ppp_year, povlines[1])]
        )
        tb_pivot[("headcount_between", ppp_year, f"{povlines[4]} and {povlines[6]}")] = (
            tb_pivot[("headcount", ppp_year, povlines[6])] - tb_pivot[("headcount", ppp_year, povlines[4])]
        )

        tb_pivot[("headcount_ratio_between", ppp_year, f"{povlines[1]} and {povlines[4]}")] = (
            tb_pivot[("headcount_ratio", ppp_year, povlines[4])] - tb_pivot[("headcount_ratio", ppp_year, povlines[1])]
        )
        tb_pivot[("headcount_ratio_between", ppp_year, f"{povlines[4]} and {povlines[6]}")] = (
            tb_pivot[("headcount_ratio", ppp_year, povlines[6])] - tb_pivot[("headcount_ratio", ppp_year, povlines[4])]
        )

    # Now, only keep headcount_between and headcount_ratio_between, and headcount_above and headcount_ratio_above
    tb_pivot = tb_pivot.loc[
        :,
        tb_pivot.columns.get_level_values(0).isin(
            [
                "country",
                "year",
                "welfare_type",
                "headcount_between",
                "headcount_ratio_between",
            ]
        ),
    ]

    # Stack table
    tb_pivot = unpivot_table(
        tb=tb_pivot.reset_index(), index=["country", "year", "welfare_type"], level=["ppp_version", "poverty_line"]
    )

    # Merge with tb
    tb = pr.merge(
        tb,
        tb_pivot,
        on=["country", "year", "welfare_type", "poverty_line", "ppp_version"],
        how="outer",
    )

    # Copy metadata to recover origin
    tb["headcount_between"] = tb["headcount_between"].copy_metadata(tb["headcount"])
    tb["headcount_ratio_between"] = tb["headcount_ratio_between"].copy_metadata(tb["headcount_ratio"])

    return tb


def pivot_and_obtain_povlines_dict(tb: Table, index: List[str], columns: List[str]) -> Table:
    """
    Pivot the table to calculate indicator more easily
    """

    # Pivot the table to calculate indicator more easily
    tb_pivot = tb.pivot(index=index, columns=columns)

    return tb_pivot


def unpivot_table(tb: Table, index: List[str], level: List[str]) -> Table:
    """
    Unpivot table, using set_index and stack
    """
    tb = (
        tb.set_index(index)  # Set the desired index, including the additional columns
        .stack(level=level, future_stack=True)  # Stack the MultiIndex columns
        .reset_index()  # Reset the index to flatten the table
    )

    return tb


def sanity_checks(
    tb: Table,
) -> Table:
    """
    Sanity checks for the table
    """

    # Define index for pivot
    index = ["country", "year", "welfare_type"]

    # Pivot and obtain the poverty lines dictionary
    tb_pivot = pivot_and_obtain_povlines_dict(tb=tb, index=index, columns=["ppp_version", "poverty_line"])

    # Reset index in tb_pivot
    tb_pivot = tb_pivot.reset_index()

    # Create lists of variables to check that depend on the poverty lines
    columns_poverty_lines = [
        "headcount",
        "headcount_ratio",
        "headcount_above",
        "headcount_ratio_above",
        "poverty_gap_index",
        "total_shortfall",
        # "watts", # not using them for analysis, since we don't plot them and they have missing values
        # "poverty_severity",
    ]

    # Create list for decile variables
    col_decile_share = []
    col_decile_thr = []
    col_decile_avg = []

    for i in range(1, 11):
        col_decile_share.append(f"decile{i}_share")
        col_decile_avg.append(f"decile{i}_avg")
        if i != 10:
            col_decile_thr.append(f"decile{i}_thr")

    for ppp_year, povlines in POVLINES_DICT.items():
        log.info(f"SANITY CHECKS FOR {ppp_year} PRICES")
        # Save the number of observations before the checks
        obs_before_checks = len(tb_pivot)
        ############################
        # Negative values
        # Create a mask to check for negative values in columns_poverty_lines + col_decile_share + col_decile_avg + col_decile_thr
        cols_to_check = [
            (col, ppp_year, povline)
            for col in columns_poverty_lines
            + col_decile_share
            + col_decile_avg
            + col_decile_thr
            + ["mean", "median", "mld", "gini", "polarization"]
            for povline in povlines
        ]
        mask = tb_pivot.loc[:, cols_to_check].lt(0).any(axis=1)
        tb_error = tb_pivot[mask].reset_index(drop=True)

        if not tb_error.empty:
            log.fatal(
                f"""There are {len(tb_error)} observations with negative values! In:
                {tabulate(tb_error[index], headers = 'keys', tablefmt = TABLEFMT)}"""
            )
            # NOTE: Check if we want to delete these observations
            # tb_pivot = tb_pivot[~mask].reset_index(drop=True)

        ############################
        # stacked values not adding up to 100%
        # Define stacked columns
        col_stacked_pct_dict = {}
        col_stacked_n_dict = {}
        # Initialize the stacked column lists representing all the possible intervals
        col_stacked_pct_all = []
        col_stacked_n_all = []
        for i in range(len(povlines)):
            # if it's the first value only continue
            if i == 0:
                continue

            # If it's the last value calculate the people between this value and the previous
            # and also the people over this poverty line (and percentages)
            else:
                varname_n = ("headcount_between", ppp_year, f"{povlines[i-1]} and {povlines[i]}")
                varname_pct = ("headcount_ratio_between", ppp_year, f"{povlines[i-1]} and {povlines[i]}")
                col_stacked_n_all.append(varname_n)
                col_stacked_pct_all.append(varname_pct)

        col_stacked_pct_all = (
            [("headcount_ratio", ppp_year, povlines[0])]
            + col_stacked_pct_all
            + [("headcount_ratio_above", ppp_year, povlines[-1])]
        )

        col_stacked_n_all = (
            [("headcount", ppp_year, povlines[0])] + col_stacked_n_all + [("headcount_above", ppp_year, povlines[-1])]
        )

        col_stacked_pct_dict[ppp_year] = {"all": col_stacked_pct_all}
        col_stacked_n_dict[ppp_year] = {"all": col_stacked_n_all}

        # Define the stacked columns for a reduced set of intervals
        col_stacked_pct_reduced = [
            ("headcount_ratio", ppp_year, povlines[1]),
            ("headcount_ratio_between", ppp_year, f"{povlines[1]} and {povlines[4]}"),
            ("headcount_ratio_between", ppp_year, f"{povlines[4]} and {povlines[6]}"),
            ("headcount_ratio_above", ppp_year, povlines[6]),
        ]
        col_stacked_n_reduced = [
            ("headcount", ppp_year, povlines[1]),
            ("headcount_between", ppp_year, f"{povlines[1]} and {povlines[4]}"),
            ("headcount_between", ppp_year, f"{povlines[4]} and {povlines[6]}"),
            ("headcount_above", ppp_year, povlines[6]),
        ]

        # Add the reduced columns to the dictionary
        col_stacked_pct_dict[ppp_year]["reduced"] = col_stacked_pct_reduced
        col_stacked_n_dict[ppp_year]["reduced"] = col_stacked_n_reduced

        # Calculate and check the sum of the stacked values
        tb_pivot["sum_pct"] = tb_pivot[col_stacked_pct_dict[ppp_year]["all"]].sum(axis=1)
        mask = (tb_pivot["sum_pct"] >= 100 + PRECISION_PERCENTAGE) | (tb_pivot["sum_pct"] <= 100 - PRECISION_PERCENTAGE)
        tb_error = tb_pivot[mask].reset_index(drop=True)

        if not tb_error.empty:
            log.warning(
                f"""{len(tb_error)} observations of all stacked values are not adding up to 100% and will be deleted:
                {tabulate(tb_error[index + ['sum_pct']], headers = 'keys', tablefmt = TABLEFMT, floatfmt=".1f")}"""
            )
            tb_pivot = tb_pivot[~mask].reset_index(drop=True)

        # For the reduced set of intervals
        tb_pivot["sum_pct"] = tb_pivot[col_stacked_pct_dict[ppp_year]["reduced"]].sum(axis=1)
        mask = (tb_pivot["sum_pct"] >= 100 + PRECISION_PERCENTAGE) | (tb_pivot["sum_pct"] <= 100 - PRECISION_PERCENTAGE)
        tb_error = tb_pivot[mask].reset_index(drop=True)

        if not tb_error.empty:
            log.warning(
                f"""{len(tb_error)} observations of the reduced set of stacked values are not adding up to 100% and will be deleted:
                {tabulate(tb_error[index + ['sum_pct']], headers = 'keys', tablefmt = TABLEFMT, floatfmt=".1f")}"""
            )
            tb_pivot = tb_pivot[~mask].reset_index(drop=True)

        ############################
        # missing poverty values (headcount, poverty gap, total shortfall)
        cols_to_check = [(col, ppp_year, povline) for col in columns_poverty_lines for povline in povlines]
        mask = (tb_pivot.loc[:, cols_to_check].isna().any(axis=1)) & (
            ~tb_pivot["country"].isin(["World (excluding China)", "World (excluding India)"])
        )
        tb_error = tb_pivot[mask].reset_index(drop=True)

        if not tb_error.empty:
            log.warning(
                f"""There are {len(tb_error)} observations with missing poverty values and will be deleted:
                {tabulate(tb_error[(index + ["headcount_ratio"])], headers = 'keys', tablefmt = TABLEFMT)}"""
            )

            tb_pivot = tb_pivot[~mask].reset_index(drop=True)

        ############################
        # Missing median, mean and gini
        for indicator in ["mean", "median", "gini"]:
            # Check if the indicator is missing
            mask = tb_pivot[(indicator, ppp_year, povlines[1])].isna()
            tb_error = tb_pivot[mask].reset_index(drop=True)

            if not tb_error.empty:
                log.info(
                    f"""There are {len(tb_error)} observations with missing {indicator}. They will be not deleted."""
                )

        ############################
        # Missing decile shares
        # Define mask as all col_decile_share columns with ppp_year = ppp_year and povlines[1]
        cols_to_check = [(col, ppp_year, povlines[1]) for col in col_decile_share]
        mask = tb_pivot[cols_to_check].isna().any(axis=1)
        tb_error = tb_pivot[mask].reset_index(drop=True)
        if not tb_error.empty:
            log.info(
                f"""There are {len(tb_error)} observations with missing decile shares. They will be not deleted."""
            )

        ############################
        # Missing decile thresholds
        cols_to_check = [(col, ppp_year, povlines[1]) for col in col_decile_thr]
        mask = tb_pivot[cols_to_check].isna().any(axis=1)
        tb_error = tb_pivot[mask].reset_index(drop=True)
        if not tb_error.empty:
            log.info(
                f"""There are {len(tb_error)} observations with missing decile thresholds. They will be not deleted."""
            )

        ############################
        # Missing decile averages
        cols_to_check = [(col, ppp_year, povlines[1]) for col in col_decile_avg]
        mask = tb_pivot[cols_to_check].isna().any(axis=1)
        tb_error = tb_pivot[mask].reset_index(drop=True)
        if not tb_error.empty:
            log.info(
                f"""There are {len(tb_error)} observations with missing decile averages. They will be not deleted."""
            )

        ############################
        # headcount monotonicity check
        # Define the headcount columns for the current ppp_year
        col_headcount = [("headcount", ppp_year, povline) for povline in povlines]
        m_check_vars = []
        for i in range(len(col_headcount)):
            if i > 0:
                check_varname = f"m_check_{i}"
                tb_pivot[check_varname] = tb_pivot[col_headcount[i]] >= tb_pivot[col_headcount[i - 1]]
                m_check_vars.append(check_varname)
        tb_pivot["check_total"] = tb_pivot[m_check_vars].all(axis=1)

        tb_error = tb_pivot[~tb_pivot["check_total"]].reset_index(drop=True)

        if not tb_error.empty:
            log.warning(
                f"""There are {len(tb_error)} observations with headcount not monotonically increasing and will be deleted:
                {tabulate(tb_error[index], headers = 'keys', tablefmt = TABLEFMT, floatfmt="0.0f")}"""
            )
            tb_pivot = tb_pivot[tb_pivot["check_total"]].reset_index(drop=True)

        ############################
        # Threshold monotonicity check
        cols_to_check = [(col, ppp_year, povlines[1]) for col in col_decile_thr]
        m_check_vars = []
        for i in range(len(cols_to_check)):
            if i > 0:
                check_varname = f"m_check_{i}"
                tb_pivot[check_varname] = tb_pivot[cols_to_check[i]] >= tb_pivot[cols_to_check[i - 1]]
                m_check_vars.append(check_varname)

        tb_pivot["check_total"] = tb_pivot[m_check_vars].all(axis=1)

        # Drop rows if columns in col_decile_thr are all null. Keep if some are null
        mask = (~tb_pivot["check_total"]) & (tb_pivot[cols_to_check].notnull().any(axis=1))

        tb_error = tb_pivot[mask].reset_index(drop=True)

        if not tb_error.empty:
            log.warning(
                f"""There are {len(tb_error)} observations with thresholds not monotonically increasing and will be deleted:
                {tabulate(tb_error[index], headers = 'keys', tablefmt = TABLEFMT)}"""
            )
            tb_pivot = tb_pivot[~mask].reset_index(drop=True)

        ############################
        # Shares monotonicity check
        cols_to_check = [(col, ppp_year, povlines[1]) for col in col_decile_share]
        m_check_vars = []
        for i in range(len(cols_to_check)):
            if i > 0:
                check_varname = f"m_check_{i}"
                tb_pivot[check_varname] = tb_pivot[cols_to_check[i]] >= tb_pivot[cols_to_check[i - 1]]
                m_check_vars.append(check_varname)

        tb_pivot["check_total"] = tb_pivot[m_check_vars].all(axis=1)

        # Drop rows if columns in col_decile_share are all null. Keep if some are null
        mask = (~tb_pivot["check_total"]) & (tb_pivot[cols_to_check].notnull().any(axis=1))
        tb_error = tb_pivot[mask].reset_index(drop=True)

        if not tb_error.empty:
            log.warning(
                f"""There are {len(tb_error)} observations with shares not monotonically increasing and will be deleted:
                {tabulate(tb_error[index], headers = 'keys', tablefmt = TABLEFMT, floatfmt=".1f")}"""
            )
            tb_pivot = tb_pivot[~mask].reset_index(drop=True)

        ############################
        # Shares not adding up to 100%

        cols_to_check = [(col, ppp_year, povlines[1]) for col in col_decile_share]
        tb_pivot["sum_pct"] = tb_pivot[cols_to_check].sum(axis=1)

        # Drop rows if columns in col_decile_share are all null. Keep if some are null
        mask = (tb_pivot["sum_pct"] >= 100 + PRECISION_PERCENTAGE) | (
            tb_pivot["sum_pct"] <= 100 - PRECISION_PERCENTAGE
        ) & (tb_pivot[cols_to_check].notnull().any(axis=1))
        tb_error = tb_pivot[mask].reset_index(drop=True).copy()

        if not tb_error.empty:
            log.warning(
                f"""{len(tb_error)} observations of shares are not adding up to 100% and will be deleted:
                {tabulate(tb_error[index + ['sum_pct']], headers = 'keys', tablefmt = TABLEFMT, floatfmt=".1f")}"""
            )
            tb_pivot = tb_pivot[~mask].reset_index(drop=True)

        ############################
        # Shares not adding up to 100% (top 1%)

        # Define columns to add up to 100%
        col_decile_share_top = ["bottom50_share", "middle40_share", "top90_99_share", "top1_share"]
        cols_to_check = [(col, ppp_year, povlines[1]) for col in col_decile_share_top]
        tb_pivot["sum_pct"] = tb_pivot[cols_to_check].sum(axis=1)

        # Drop rows if columns in col_decile_share_top are all null. Keep if some are null
        mask = (tb_pivot["sum_pct"] >= 100 + PRECISION_PERCENTAGE) | (
            tb_pivot["sum_pct"] <= 100 - PRECISION_PERCENTAGE
        ) & (tb_pivot[cols_to_check].notnull().any(axis=1))
        tb_error = tb_pivot[mask].reset_index(drop=True).copy()

        if not tb_error.empty:
            log.warning(
                f"""{len(tb_error)} observations of shares (with top 1%) are not adding up to 100% and will be converted to null:
                {tabulate(tb_error[index + ['sum_pct']], headers = 'keys', tablefmt = TABLEFMT, floatfmt=".1f")}"""
            )
            # Make columns None if mask is True
            tb_pivot.loc[mask, [("top90_99_share", ppp_year, povlines[1]), ("top1_share", ppp_year, povlines[1])]] = (
                pd.NA
            )

        ############################
        # delete columns created for the checks
        tb_pivot = tb_pivot.drop(columns=m_check_vars + ["m_check_1", "check_total", "sum_pct"])

        obs_after_checks = len(tb_pivot)
        log.info(f"Sanity checks deleted {obs_before_checks - obs_after_checks} observations for {ppp_year} PPPs.")

    # Restore the format of the table
    tb = unpivot_table(tb=tb_pivot, index=index, level=["ppp_version", "poverty_line"])

    return tb


def inc_or_cons_data(tb: Table) -> Tuple[Table, Table]:
    """
    Separate income and consumption data
    """

    # Make a copy of the table
    tb_spells = tb.copy()
    tb_no_spells = tb.copy()

    # Generate tb_inc_spells and tb_cons_spells
    tb_inc_spells = tb_spells[tb_spells["welfare_type"] == "income"].reset_index(drop=True)
    tb_cons_spells = tb_spells[tb_spells["welfare_type"] == "consumption"].reset_index(drop=True)

    # Drop the survey_comparability column for tb_no_spells
    tb_no_spells = tb_no_spells.drop(columns=["survey_comparability"])

    # Generate tb_inc_no_spells and tb_cons_no_spells
    tb_inc_no_spells = tb_no_spells[tb_no_spells["welfare_type"] == "income"].reset_index(drop=True)
    tb_cons_no_spells = tb_no_spells[tb_no_spells["welfare_type"] == "consumption"].reset_index(drop=True)

    # Create tb_no_spells_smooth, which cleans tb_no_spells, by removing jumps generated by changes in welfare_type
    tb_no_spells_smooth = create_smooth_inc_cons_series(tb_no_spells)

    # # TODO: Come back to fix this
    # check_jumps_in_grapher_dataset(tb_no_spells_smooth)

    # Add the column table, identifying the type of table to use in Grapher
    tb_spells["table"] = "Income or consumption with spells"
    tb_inc_spells["table"] = "Income with spells"
    tb_cons_spells["table"] = "Consumption with spells"
    tb_no_spells["table"] = "Income or consumption"
    tb_inc_no_spells["table"] = "Income"
    tb_cons_no_spells["table"] = "Consumption"
    tb_no_spells_smooth["table"] = "Income or consumption consolidated"

    # Also, rename welfare_type to "Income or consumption" for tb_no_spells_smooth
    tb_no_spells_smooth["welfare_type"] = "Income or consumption"

    # Concatenate all these tables
    tb = pr.concat(
        [
            tb_spells,
            tb_inc_spells,
            tb_cons_spells,
            tb_no_spells,
            tb_inc_no_spells,
            tb_cons_no_spells,
        ],
        ignore_index=True,
    )

    return tb, tb_no_spells_smooth


def create_smooth_inc_cons_series(tb: Table) -> Table:
    """
    Construct an income and consumption series that is a combination of the two.
    """

    tb = tb.copy()

    # Pivot and obtain the poverty lines dictionary
    tb = pivot_and_obtain_povlines_dict(
        tb=tb,
        index=["country", "year", "welfare_type"],
        columns=["ppp_version", "poverty_line", "decile"],
    )

    # Reset index in tb_both_inc_and_cons
    tb = tb.reset_index()

    # Sort values
    tb = tb.sort_values(by=["country", "year", "welfare_type"], ignore_index=True)

    # Flag duplicates per year – indicating multiple welfare_types
    tb["duplicate_flag"] = tb.duplicated(subset=[("country", "", "", ""), ("year", "", "", "")], keep=False)

    # Create a boolean column that is true if each country has only income or consumption
    tb["only_inc_or_cons"] = tb.groupby(["country"])["welfare_type"].transform(lambda x: x.nunique() == 1)

    # Select only the rows with only income or consumption
    tb_only_inc_or_cons = tb[tb["only_inc_or_cons"]].reset_index(drop=True)

    # Create a table with the rest
    tb_both_inc_and_cons = tb[~tb["only_inc_or_cons"]].reset_index(drop=True)

    # Create a list of the countries with both income and consumption in the series
    countries_inc_cons = list(tb_both_inc_and_cons["country"].unique())

    # Assert that the countries with both income and consumption are expected
    unexpected_countries = set(countries_inc_cons) - set(COUNTRIES_WITH_INCOME_AND_CONSUMPTION)
    missing_countries = set(COUNTRIES_WITH_INCOME_AND_CONSUMPTION) - set(countries_inc_cons)
    assert not unexpected_countries and not missing_countries, log.fatal(
        f"Unexpected countries with both income and consumption: {unexpected_countries}. "
        f"Missing expected countries: {missing_countries}."
    )

    # Define empty table to store the smoothed series
    tb_both_inc_and_cons_smoothed = Table()
    for country in countries_inc_cons:
        # Filter country
        tb_country = tb_both_inc_and_cons[tb_both_inc_and_cons["country"] == country].reset_index(drop=True)

        # Save the max_year for the country
        max_year = tb_country["year"].max()

        # Define last_welfare_type for income and consumption. If both, list is saved as ['income', 'consumption']
        last_welfare_type = list(tb_country[tb_country["year"] == max_year]["welfare_type"].unique())
        last_welfare_type.sort()

        # Count how many times welfare_type switches from income to consumption and vice versa
        number_of_welfare_series = (
            (tb_country["welfare_type"] != tb_country["welfare_type"].shift(1).fillna("")).astype(int).cumsum().max()
        )

        # If there are only two welfare series, use both, except for countries where we have to choose one
        if number_of_welfare_series == 2:
            # assert if last_welfare type values are expected
            if country in ["Armenia", "Belarus", "Kyrgyzstan", "North Macedonia", "Peru"]:
                if country in ["Armenia", "Belarus", "Kyrgyzstan"]:
                    welfare_expected = ["consumption"]
                    assert len(last_welfare_type) == 1 and last_welfare_type == welfare_expected, log.fatal(
                        f"{country} has unexpected values of welfare_type: {last_welfare_type} instead of {welfare_expected}."
                    )

                elif country in ["North Macedonia", "Peru"]:
                    assert len(last_welfare_type) == 1 and last_welfare_type == ["income"], log.fatal(
                        f"{country} has unexpected values of welfare_type: {last_welfare_type} instead of ['income']"
                    )

                tb_country = tb_country[tb_country["welfare_type"].isin(last_welfare_type)].reset_index(drop=True)

        # With Turkey I also want to keep both series, but there are duplicates for some years
        elif country in ["Turkey"]:
            welfare_expected = ["income"]
            assert len(last_welfare_type) == 1 and last_welfare_type == welfare_expected, log.fatal(
                f"{country} has unexpected values of welfare_type: {last_welfare_type} instead of {welfare_expected}"
            )

            tb_country = tb_country[
                (~tb_country["duplicate_flag"]) | (tb_country["welfare_type"].isin(last_welfare_type))
            ].reset_index(drop=True)

        # With Russia, though the last welfare type is income, I want to keep consumption, being the longest series
        elif country in ["Russia"]:
            welfare_expected = ["consumption"]
            assert len(last_welfare_type) == 1 and last_welfare_type != welfare_expected, log.fatal(
                f"For {country} we expect to use {welfare_expected}, which should be different from the last welfare type: {last_welfare_type}"
            )

            tb_country = tb_country[tb_country["welfare_type"].isin(welfare_expected)].reset_index(drop=True)

        # These are countries with both income and consumption as the last welfare type, so I decide case by case
        elif country in ["Haiti", "Philippines", "Romania", "Saint Lucia"]:
            welfare_expected = ["consumption", "income"]
            assert len(last_welfare_type) == 2 and last_welfare_type == welfare_expected, log.fatal(
                f"{country} has unexpected values of welfare_type: {last_welfare_type} instead of {welfare_expected}"
            )
            if country in ["Haiti", "Romania", "Saint Lucia"]:
                tb_country = tb_country[tb_country["welfare_type"] == "income"].reset_index(drop=True)
            else:
                tb_country = tb_country[tb_country["welfare_type"] == "consumption"].reset_index(drop=True)

        else:
            # Here I keep the most recent welfare type
            if country in ["Albania", "Ukraine"]:
                welfare_expected = ["consumption"]
                assert len(last_welfare_type) == 1 and last_welfare_type == welfare_expected, log.fatal(
                    f"{country} has unexpected values of welfare_type: {last_welfare_type} instead of {welfare_expected}."
                )
            else:
                welfare_expected = ["income"]
                assert len(last_welfare_type) == 1 and last_welfare_type == welfare_expected, log.fatal(
                    f"{country} has unexpected values of welfare_type: {last_welfare_type} instead of {welfare_expected}."
                )

            tb_country = tb_country[tb_country["welfare_type"].isin(last_welfare_type)].reset_index(drop=True)

        tb_both_inc_and_cons_smoothed = pr.concat([tb_both_inc_and_cons_smoothed, tb_country])

    # Drop the columns created in this function
    tb_both_inc_and_cons_smoothed = tb_both_inc_and_cons_smoothed.drop(columns=["only_inc_or_cons", "duplicate_flag"])
    tb_only_inc_or_cons = tb_only_inc_or_cons.drop(columns=["only_inc_or_cons", "duplicate_flag"])

    # Restore the format of the table
    tb_both_inc_and_cons_smoothed = unpivot_table(
        tb=tb_both_inc_and_cons_smoothed,
        index=["country", "year", "welfare_type"],
        level=["ppp_version", "poverty_line", "decile"],
    )
    tb_only_inc_or_cons = unpivot_table(
        tb=tb_only_inc_or_cons,
        index=["country", "year", "welfare_type"],
        level=["ppp_version", "poverty_line", "decile"],
    )

    tb_inc_or_cons = pr.concat([tb_only_inc_or_cons, tb_both_inc_and_cons_smoothed], ignore_index=True)

    return tb_inc_or_cons


def check_jumps_in_grapher_dataset(tb: Table) -> None:
    """
    Check for jumps in the dataset, which can be caused by combining income and consumption estimates for one country series.
    """
    tb = tb.copy()

    # Pivot and obtain the poverty lines dictionary
    tb = pivot_and_obtain_povlines_dict(
        tb=tb,
        index=["country", "year", "welfare_type"],
        columns=["ppp_version", "poverty_line"],
    )

    # Reset index in tb
    tb = tb.reset_index()

    # For each country, year, welfare_type and reporting_level, check if the difference between the columns is too high

    for ppp_year, povlines in POVLINES_DICT.items():
        # Define columns to check: all the headcount ratio columns
        cols_to_check = [("headcount_ratio", ppp_year, povline) for povline in povlines]
        for col in cols_to_check:
            # Create a new column, shift_col, that is the same as col but shifted one row down for each country
            tb["shift_col"] = tb.groupby(["country"])[col].shift(1)

            # Create shift_year column
            tb["shift_year"] = tb.groupby(["country"])["year"].shift(1)

            # Create shift_welfare_type column
            tb["shift_welfare_type"] = tb.groupby(["country"])["welfare_type"].shift(1)

            # Calculate the difference between col and shift_col
            tb["check_diff_column"] = tb[col] - tb["shift_col"]

            # Calculate the difference between years
            tb["check_diff_year"] = tb["year"] - tb["shift_year"]

            # Calculate if the welfare type is the same
            tb["check_diff_welfare_type"] = tb["welfare_type"] == tb["shift_welfare_type"]

            # Check if the difference is too high
            mask = (
                (abs(tb["check_diff_column"].fillna(0)) > 10)
                & (tb["check_diff_year"].fillna(0) <= 5)
                & (~tb["check_diff_welfare_type"].fillna(False))
            )
            tb_error = tb[mask].reset_index(drop=True)

            if not tb_error.empty:
                log.warning(
                    f"""There are {len(tb_error)} observations with abnormal jumps for {col}:
                    {tabulate(tb_error[['ppp_version', 'country', 'year', 'reporting_level', col, 'check_diff_column', 'check_diff_year']].sort_values('year').reset_index(drop=True), headers = 'keys', tablefmt = TABLEFMT, floatfmt=".1f")}"""
                )
                # tb = tb[~mask].reset_index(drop=True)

    # Drop the columns created for the check
    tb = tb.drop(
        columns=[
            "shift_col",
            "shift_year",
            "shift_welfare_type",
            "check_diff_column",
            "check_diff_year",
            "check_diff_welfare_type",
        ]
    )

    return None


def survey_count(tb: Table) -> Table:
    """
    Create survey count indicator, by counting the number of surveys available for each country in the past decade
    """
    tb = tb.copy()

    # Remove regions from the table
    tb_survey = tb[~tb["country"].isin(REGIONS_LIST)].reset_index(drop=True).copy()

    # Obtain the value of the second key in INTERNATIONAL_POVERTY_LINES
    # This is the value of the poverty line for the current year
    ipl_current = INTERNATIONAL_POVERTY_LINES[list(INTERNATIONAL_POVERTY_LINES.keys())[1]]

    # Filter for the current value
    tb_survey = tb_survey[tb_survey["poverty_line"] == ipl_current].reset_index(drop=True)

    min_year = int(tb_survey["year"].min())
    max_year = int(tb_survey["year"].max())
    year_list = list(range(min_year, max_year + 1))
    country_list = list(tb_survey["country"].unique())

    # Create two tables with all the years and entities
    year_tb_survey = Table(year_list)
    entity_tb_survey = Table(country_list)

    # Make a cartesian product of both dataframes: join all the combinations between all the entities and all the years
    cross = pr.merge(entity_tb_survey, year_tb_survey, how="cross")
    cross = cross.rename(columns={"0_x": "country", "0_y": "year"})

    # Merge cross and tb_survey, to include all the possible rows in the dataset
    tb_survey = pr.merge(cross, tb_survey[["country", "year"]], on=["country", "year"], how="left", indicator=True)

    # Mark with 1 if there are surveys available, 0 if not (this is done by checking if the row is in both datasets)
    tb_survey["survey_available"] = 0
    tb_survey.loc[tb_survey["_merge"] == "both", "survey_available"] = 1

    # Sum for each entity the surveys available for the previous 9 years and the current year
    tb_survey["surveys_past_decade"] = (
        tb_survey["survey_available"]
        .groupby(tb_survey["country"], sort=False)
        .rolling(min_periods=1, window=10)
        .sum()
        .values
    )

    # Copy metadata
    tb_survey["surveys_past_decade"] = tb_survey["surveys_past_decade"].copy_metadata(tb["headcount"])

    # Keep columns needed
    tb_survey = tb_survey[["country", "year", "surveys_past_decade"]]

    # Define ppp_version and poverty_line columns, empty
    tb_survey["ppp_version"] = pd.NA
    tb_survey["poverty_line"] = pd.NA

    # Merge with original table
    tb = pr.merge(tb_survey, tb, on=["country", "year", "ppp_version", "poverty_line"], how="outer")

    return tb


def drop_columns(tb: Table) -> Table:
    """
    Drop columns not needed
    """

    # Remove columns
    tb = tb.drop(
        columns=[
            "reporting_pop",
            "is_interpolated",
        ]
    )

    return tb


def add_region_definitions(tb: Table, tb_region_definitions: Table) -> Table:
    """
    Add region definitions to the main table
    """

    tb = tb.copy()
    tb_region_definitions = tb_region_definitions.copy()

    # Merge with the main table
    tb = pr.merge(tb, tb_region_definitions, on=["country", "year"], how="outer")

    return tb


def show_not_dimensional_data_once(tb: Table) -> Table:
    """
    Make all the columns that are not dimensional (do not depend on dimensions) to be shown once.
    """

    return tb


def make_distributional_indicators_long(tb: Table) -> Table:
    """
    Convert decile1, ..., decile10 and decile1_thr, ..., decile9_thr to a long format.
    """
    tb_base = tb.copy()

    # Extract both values from INTERNATIONAL_POVERTY_LINES. They are the values of the dictionary
    ipl_list = list(INTERNATIONAL_POVERTY_LINES.values())

    # Filter only for values in the list
    tb_base = tb_base[tb_base["poverty_line"].isin(ipl_list)].reset_index(drop=True)

    # Define index columns
    index_columns = ["country", "year", "welfare_type", "ppp_version"]

    # SHARE
    # Define share columns
    share_columns = [f"decile{i}_share" for i in range(1, 11)]
    tb_share = tb_base.melt(
        id_vars=index_columns,
        value_vars=share_columns,
        var_name="decile",
        value_name="share",
    )

    # THRESHOLD
    # Define threshold columns
    thr_columns = [f"decile{i}_thr" for i in range(1, 10)]
    tb_thr = tb_base.melt(
        id_vars=index_columns,
        value_vars=thr_columns,
        var_name="decile",
        value_name="thr",
    )

    # AVERAGE
    # Define average columns
    avg_columns = [f"decile{i}_avg" for i in range(1, 11)]
    tb_avg = tb_base.melt(
        id_vars=index_columns,
        value_vars=avg_columns,
        var_name="decile",
        value_name="avg",
    )

    # Merge newly created tables
    tb_distributional = pr.multi_merge(
        [tb_share, tb_thr, tb_avg],
        on=index_columns + ["decile"],
        how="outer",
    )

    # Remove "decile" from the decile column
    tb_distributional["decile"] = tb_distributional["decile"].str.replace("decile", "")

    # Do the same with "_share", "_thr", and "_avg"
    for indicator in ["share", "thr", "avg"]:
        tb_distributional["decile"] = tb_distributional["decile"].str.replace(f"_{indicator}", "")

    # Group the rows by country, year, welfare_type, ppp_version, and decile
    tb_distributional = (
        tb_distributional.groupby(
            index_columns + ["decile"],
            as_index=False,
        )
        .agg(
            {
                "share": "first",
                "thr": "first",
                "avg": "first",
            }
        )
        .reset_index(drop=True)
    )

    # Create an empty decile column in tb
    tb["decile"] = pd.NA

    # Concatenate tb and tb_distributional
    tb = pr.concat([tb, tb_distributional], ignore_index=True)

    # Remove share_columns and threshold_columns
    tb = tb.drop(columns=share_columns + thr_columns + avg_columns, errors="raise")

    return tb


def make_relative_poverty_long(tb: Table) -> Table:
    """
    Convert relative poverty columns to a long format.
    """
    tb_relative = tb.copy()

    # Extract both values from INTERNATIONAL_POVERTY_LINES. They are the values of the dictionary
    ipl_list = list(INTERNATIONAL_POVERTY_LINES.values())

    # Filter only for values in the list
    tb_relative = tb_relative[tb_relative["poverty_line"].isin(ipl_list)].reset_index(drop=True)

    # Define index columns
    index_columns = ["country", "year", "welfare_type", "ppp_version"]

    # Define relative poverty columns. They are all the columns that contain "_median"
    rel_pov_columns = [col for col in tb.columns if "_median" in col]

    # Melt the table
    tb_relative = tb_relative.melt(
        id_vars=index_columns,
        value_vars=rel_pov_columns,
        var_name="indicator",
        value_name="value",
    )

    # Split the indicator column into two columns: indicator and poverty_line. Poverty line would be in the format "40_median", "50_median", or "60_median"
    tb_relative[["indicator", "poverty_line"]] = tb_relative["indicator"].str.extract(
        r"(.+?)_(\d+_median)", expand=True
    )

    # In poverty_line, replace "_median" with "% of median"
    tb_relative["poverty_line"] = tb_relative["poverty_line"].str.replace("_median", "% of median")

    # Make tb_relative wide, by pivoting the indicator column
    tb_relative = tb_relative.pivot(
        index=index_columns + ["poverty_line"],
        columns="indicator",
        values="value",
    ).reset_index()

    # Concatenate with original table
    tb = pr.concat([tb, tb_relative], ignore_index=True)

    # Drop the columns that are not needed
    tb = tb.drop(columns=rel_pov_columns, errors="raise")

    return tb
