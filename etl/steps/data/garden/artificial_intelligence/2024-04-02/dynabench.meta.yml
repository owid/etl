# NOTE: To learn more about the fields, hover over their names.
definitions:
  common:
    processing_level: minor
    description_processing: >
      We mapped the benchmarks to their respective domains based on a review of each benchmark's primary focus and the specific capabilities it tests within AI systems:

      - MNIST was mapped to "Handwriting recognition", as it tests AI systems' ability to recognize and classify handwritten digits, a fundamental task in the domain of digital image processing.
      - GLUE was categorized under "Broad language understanding" due to its assessment of models across a variety of linguistic tasks, highlighting the general capabilities of AI in understanding human language.
      - ImageNet was categorized as "Image classification", focusing on the ability of AI systems to accurately identify and categorize images into predefined classes, showcasing the advancements in visual perception.
      - SQuAD 1.1 and SQuAD 2.0 were distinguished as "Text-based question answering" and "Comprehensive reading comprehension" respectively. While both benchmarks evaluate reading comprehension, SQuAD 2.0 adds an extra layer of complexity with the introduction of unanswerable questions, demanding deeper understanding and reasoning from AI models.
      - BBH was aligned with "Creative reasoning and problem solving", as it challenges AI with tasks that require not just logical reasoning but also creative thinking, simulating complex problem-solving scenarios.
      - Switchboard was associated with "Conversational language processing" due to its focus on transcribing and understanding human speech within a conversational context, evaluating AI's ability to process and respond to spoken language.
      - MMLU was placed in "Language based knowledge tests", given its assessment across multiple disciplines and topics, requiring a broad and comprehensive understanding of language.
      - HellaSwag was mapped to "Contextual reasoning and prediction" for its evaluation of AI's ability to predict logical continuations within given contexts, testing commonsense reasoning and understanding.
      - HumanEval was categorized under "Programming and code generation", focusing on AI's capability to understand programming languages and generate code that solves specific problems, highlighting skills in logical thinking and algorithmic problem-solving.
      - SuperGLUE was designated as "Nuanced language interpretation" due to its advanced set of linguistic tasks that require deep understanding, reasoning, and interpretation of text, pushing the boundaries of what AI can comprehend.
      - GSK8k was mapped to "Mathematical reasoning and logic", as it tests AI on solving mathematical problems that involve reasoning and logical deduction, reflecting capabilities in numerical understanding and problem-solving.
    unit: ''
    short_unit: ''
    presentation:
      topic_tags:
        - Artificial Intelligence


# Learn more about the available fields:
# http://docs.owid.io/projects/etl/architecture/metadata/reference/
dataset:
  update_period_days: 365

tables:
  dynabench:
    variables:
      performance:
        title: Performance
        description_short: Test scores of AI relative to human performance. Human performance, as the benchmark, is set to zero. The capability of each AI system is normalized to an initial performance of - 1.

      assessment_domain:
        title: Assessment domain
        description_short: The domain that the AI performance benchmark assesses.
