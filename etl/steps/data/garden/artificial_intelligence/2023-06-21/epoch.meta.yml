dataset:
  title: Parameter, Compute and Data Trends in Machine Learning (Epoch, 2023)
  description: >-
    The affiliation of the research team building a particular notable AI system was classified according to the following:

    — Academia: 100% of researchers affiliated with academia

    — Collaboration, Academia-majority: 71–99% affiliated with academia

    — Collaboration: 30–70% affiliated with academia

    — Collaboration, Industry-majority: 71–99% affiliated with industry

    — Industry: 100% of researchers affiliated with industry


    This data corresponds to the "Organization Categorization" and "Domain" columns in the primary source data spreadsheet.


    The authors selected the AI systems for inclusion based on the following necessary criteria:

    — Have an explicit learning component

    — Showcase experimental results

    — Advance the state of the art


    In addition, the systems had to meet at least one of the following notability criteria:

    — Paper has more than 1000 citations

    — Historical importance

    — Important state-of-the-art advance

    — Deployed in a notable context


    The authors note that: "For new models (from 2020 onward) it is harder to assess these criteria, so we fall back to a subjective selection. We refer to models meeting our selection criteria as 'milestone models.'"
  licenses:
  - name: Public domain
  sources:
  - name: Epoch (2023)
    url: https://epochai.org/mlinputs/visualization
    date_accessed: '2023-06-21'
    publication_date: '2023-06-06'
    publication_year: 2023
    published_by: 'Epoch, Parameter, Compute and Data Trends in Machine Learning.
      Published online at epochai.org. Retrieved from: https://epochai.org/mlinputs/visualization'
tables:
  epoch:
    variables:
      system:
        title: Name of the model
        unit: ''
      domain:
        title: Domain
        unit: ''
      publication_date:
        title: Publication date
        unit: 'date'
      organization_categorization:
        title: Researcher affiliation
        unit: ''
      parameters:
        title: Number of parameters
        description: >
          The number of parameters in AI models refers to the total count of learnable variables or weights that the model contains. Parameters are the internal variables that the model adjusts during the training process to optimize its performance and make predictions based on the input data.


          In the context of deep learning models, which are a type of AI model, parameters are typically associated with the connections between the neurons or units in the neural network. Each connection has a weight associated with it, and these weights collectively represent the parameters of the model.


          The number of parameters in a model depends on its architecture and complexity. Deep learning models often have multiple layers, each containing numerous neurons or units. The connections between these units contribute to the overall parameter count. Additionally, other components such as convolutional filters, recurrent connections, and attention mechanisms also add to the parameter count.


          The number of parameters in AI models has a significant impact on model capacity and its ability to learn complex patterns from data. A larger number of parameters can allow the model to capture more intricate relationships and potentially achieve higher accuracy. However, it also increases the risk of overfitting, where the model becomes too specialized to the training data and performs poorly on unseen examples. Balancing the number of parameters to avoid overfitting while maintaining sufficient model capacity is a crucial consideration in model design.


          In recent years, AI models with billions or even trillions of parameters have been developed. These models, known as "giant models," have demonstrated state-of-the-art performance in various tasks but require substantial computational resources for training and inference. Efficiently managing and training models with a large number of parameters is an active area of research in the AI community.
        unit: ''
        display:
          numDecimalPlaces: 0
          zeroDay: '1949-01-01'
          yearIsDay: true
      training_compute__flop:
        title: Training compute (FLOP)
        unit: 'FLOP'
        display:
          title: Training compute
          numDecimalPlaces: 0
          zeroDay: '1949-01-01'
          yearIsDay: true
      training_dataset_size__datapoints:
        title: Training dataset size
        unit: 'datapoints'
        description: >
          Training data size refers to the amount or quantity of data that is used to train an AI model, indicating the number of examples or instances available for the model to learn from.


          Imagine you're teaching a computer to recognize different types of fruits. The training data size would refer to the number of fruit images you show to the computer during the training process. If you show it 100 images of various fruits, then the training data size would be 100. The more training data you provide, the better the computer can learn to identify different fruits accurately.
        display:
          numDecimalPlaces: 0
          zeroDay: '1949-01-01'
          yearIsDay: true
      training_time__hours:
        title: Training time
        unit: 'hours'
        display:
          numDecimalPlaces: 0
          zeroDay: '1949-01-01'
          yearIsDay: true
      equivalent_training_time__hours:
        title: Equivalent training time
        unit: 'hours'
        display:
          numDecimalPlaces: 0
          zeroDay: '1949-01-01'
          yearIsDay: true
      training_computation_petaflop:
        title: Training compute (petaFLOP)
        unit: 'petaFLOP'
        description: >
          Training computation, often measured in total FLOP (floating-point operations), refers to the total number of computer operations used to train an AI system. One FLOP is equivalent to one addition, subtraction, multiplication, or division of two decimal numbers, and one petaFLOP equals one quadrillion (10^15) FLOP.


          The AI systems shown here were built using machine learning and deep learning methods. These involve complex mathematical calculations that require significant computational resources. Training these systems generally involves feeding large amounts of data through various layers and nodes and adjusting internal system parameters over numerous iterations to optimize the system’s performance.


          The training computation used can vary depending on factors such as the size of the dataset, size and complexity of the system architecture, and the level of parallelism used during training, among other reasons.
        display:
            title: Training compute
            numDecimalPlaces: 0
            zeroDay: '1949-01-01'
            yearIsDay: true


