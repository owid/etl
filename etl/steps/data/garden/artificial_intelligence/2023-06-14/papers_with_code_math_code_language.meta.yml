dataset:
  title: Math, Coding and Language Benchmarks (Papers With Code, 2023)
  description: &desc |
      The dataset includes two benchmarks of AI models performance: MATH and APPS and MMLU.
      The MATH benchmark consists of 12,500 challenging competition mathematics problems.
      The problems in the MATH benchmark are designed to test problem-solving skills and measure
      performance in mathematics.
      The APPS benchmark focuses on coding ability and problem-solving in a natural language context.
      The dataset comprises problems collected from open-access coding websites such as Codeforces and Kattis.
      The problems range in difficulty from introductory to collegiate competition level.
      The benchmark aims to mimic how human programmers are evaluated by posing coding problems in unrestricted
      natural language and assessing the correctness of solutions.
      Massive Multitask Language Understanding (MMLU) framework, form a comprehensive evaluation system.
      MMLU is designed to measure the knowledge acquired during pretraining by assessing models in zero-shot
      and few-shot settings. The benchmark covers a wide range of subjects across STEM, the humanities, the social
      sciences, and more.
  licenses:
  - name: CC BY-SA 4.0
    url: https://creativecommons.org/licenses/by-sa/4.0/
  sources:
  - name: Papers With Code (2023)
    description: *desc
    url: https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu
    source_data_url: https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu
    date_accessed: '2023-06-14'
    publication_year: 2023
    published_by: Papers With Code
tables:
  papers_with_code_math_code_language:
    variables:
      performance_humanities:
        title: MMLU Performance on Humanities
        unit: ''
      name:
        title: Model name
        unit: ''
      performance_stem:
        title: MMLU performance (STEM)
        unit: '%'
        short_unit: '%'
        display:
          numDecimalPlaces: 0
          zeroDay: '2019-01-01'
          yearIsDay: true
      performance_social_sciences:
        title: MMLU performance (Social Sciences)
        unit: '%'
        short_unit: '%'
        display:
          numDecimalPlaces: 0
          zeroDay: '2019-01-01'
          yearIsDay: true
      performance_other:
        title: MMLU performance (Other)
        unit: '%'
        short_unit: '%'
        display:
          numDecimalPlaces: 0
          zeroDay: '2019-01-01'
          yearIsDay: true
      performance_math:
        title: MATH performance
        unit: '%'
        short_unit: '%'
        display:
          numDecimalPlaces: 0
          zeroDay: '2019-01-01'
      performance_code_any_competition:
        title: Coding APPS performance (any competition)
        unit: '%'
        short_unit: '%'
        display:
          numDecimalPlaces: 0
          zeroDay: '2019-01-01'
          yearIsDay: true
      performance_code_any_interview:
        title: Coding APPS performance (any interview)
        unit: '%'
        short_unit: '%'
        display:
          numDecimalPlaces: 0
          zeroDay: '2019-01-01'
          yearIsDay: true
      performance_language_average:
        title: MMLU performance (Average)
        unit: '%'
        short_unit: '%'
        display:
          numDecimalPlaces: 0
          zeroDay: '2019-01-01'
          yearIsDay: true
