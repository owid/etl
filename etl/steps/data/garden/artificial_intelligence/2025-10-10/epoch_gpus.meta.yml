# NOTE: To learn more about the fields, hover over their names.
definitions:
  common:
    processing_level: major
    presentation:
      topic_tags:
        - Artificial Intelligence
    display:
      zeroDay: '2000-01-01'
      yearIsDay: true
      numDecimalPlaces: 0
    description_processing: |-
      - Reporting a time series of AI investments in nominal prices (i.e., without adjusting for inflation) means it makes little sense to compare observations across time; it is therefore not very useful. To make comparisons across time possible, one has to take into account that prices change (e.g., there is inflation).
      - It is not obvious how to adjust this time series for inflation, and we debated it at some length within our team.
      - It would be straightforward to adjust the time series for price changes if we knew the prices of the specific goods and services that these investments purchased. This would make it possible to calculate a volume measure of AI investments, and it would tell us how much these investments bought. But such a metric is not available. While a comprehensive price index is not available, we know that the cost for some crucial AI technology has fallen rapidly in price.
      - In the absence of a comprehensive price index that captures the price of AI-specific goods and services, one has to rely on one of the available metrics for the price of a bundle of goods and services. In the end we decided to use the US Consumer Price Index (CPI).
      - The US CPI does not provide us with a volume measure of AI goods and services, but it does capture the opportunity costs of these investments. The inflation adjustment of this time series of AI investments therefore lets us understand the size of these investments relative to whatever else these sums of money could have purchased.

dataset:
  update_period_days: 365

tables:
  epoch_gpus:
    variables:

      manufacturer:
        title: Manufacturer
        unit: ''
        short_unit: ''
        description_short: The company that produced the hardware.

      comp_performance_per_dollar:
        title: Computational performance per dollar
        unit: FLOP/s/$
        description_short: Hardware computational performance shown in [floating-point operations](#dod:flop) per second (FLOP/s) per US dollar, adjusted for inflation.
        description_key:
          - Measures how much computational performance GPUs deliver for each dollar spent.
          - A GPU is a processor built for parallel computation; it now powers most AI training because it can run many calculations at once.
          - Includes only GPUs used to train notable AI models or clearly designed for ML workloads; TPUs and CPUs are excluded.
          - Performance comes from benchmarks of AI-relevant tasks and is compared with the GPU launch price.
          - Reported as floating-point operations per second (FLOP/s) per US dollar, adjusted for inflation; amounts are in constant 2024 US$ using the U.S. Consumer Price Index (CPI).
          - FLOP/s values use 32-bit full-precision math ("FP32"), which stores numbers to about 7 decimal places. Using FP32 keeps results comparable across hardware; faster, lower-precision formats (FP16, BF16, INT8) exist but are not used here.
          - As GPUs have delivered more compute per dollar over time, the cost of training AI systems has fallen.
        presentation:
          grapher_config:
            note: FLOP/s values refer to 32-bit (full) precision. Data is expressed in constant 2024 US$. Inflation adjustment is based on the US Consumer Price Index (CPI).
